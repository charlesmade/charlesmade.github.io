{"meta":{"title":"Zeco's blog","subtitle":null,"description":null,"author":"Zeco","url":"http://zeco.oschina.io"},"pages":[{"title":"关于作者","date":"un00fin00","updated":"un00fin00","comments":true,"path":"about/index.html","permalink":"http://zeco.oschina.io/about/index.html","excerpt":"","text":""},{"title":"标签云","date":"un00fin00","updated":"un00fin00","comments":true,"path":"tags/index.html","permalink":"http://zeco.oschina.io/tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"un11fin11","updated":"un11fin11","comments":true,"path":"categories/index.html","permalink":"http://zeco.oschina.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"LinuxCon引发的思考","slug":"futrue_road","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2017/06/28/futrue_road/","link":"","permalink":"http://zeco.oschina.io/2017/06/28/futrue_road/","excerpt":"2017年的夏天，LinuxCon第一次在中国召开，一年之前，我对这个消息一点反应也没有，现在却突兀的觉得有些触动，似乎某些不起眼的东西就发生在你身边，这世界上没有谁能知道这对未来代表了什么","text":"2017年的夏天，LinuxCon第一次在中国召开，一年之前，我对这个消息一点反应也没有，现在却突兀的觉得有些触动，似乎某些不起眼的东西就发生在你身边，这世界上没有谁能知道这对未来代表了什么，我不知道，你也不知道，但世间的事总是这样，不是等你准备好了再发生，而是有着它自己的规律，就那么生长着，盛放着，爆发着，然后死亡。 闲扯了一堆没头没脑的话，就像我近来混沌的头脑，里面填了一些东西，反倒不如当初清醒，不知道自己需要什么了。 然而，今天突然看到linus在和Dirk Hohndel的炉边谈话里的这么一句话 For me, I was always self-motivated and knew what I wanted to do. I was never told what I should look at doing. I’m not sure my example is the right thing for people to follow. There are a ton of open source projects and, if you are a beginning programmer, find something you’re interested in that you can follow for more than just a few weeks. Get to know the code so well that you get to the point where you are an expert on a code piece. It doesn’t need to be the whole project. No one is an expert on the whole kernel, but you can know an area well. If you can be part of a community and set up patches, it’s not just about the coding, but about the social aspect of open source. You make connections and improve yourself as a programmer. You are basically showing off – I made these improvements, I’m capable of going far in my community or job. You’ll have to spend a certain amount of time to learn a project, but there’s a huge upside – not just from a career aspect, but having an amazing project in your life. 对于其中it’s not just about the coding，you make connections 这两句话话，感触尤深。 毫无波澜的工作生活往往让我们忘记一些东西，有些时候我们通过游戏，狂欢来消磨忘记它们，但这世界上最稳固的东西就是我们每天都要面对的日出日落，不论怎么逃避，他就在那里等着你回去，等着你去面对他。 我也模糊的明晰了自己所需要的东西，我不需要在每一个方面做得完美，但需要在某一个方面，做到令自己自豪的成绩，最后，一句话送给自己和看到这篇文章的你，the day and night, not only for small life,but for a gaint soul.","categories":[{"name":"闲扯","slug":"闲扯","permalink":"http://zeco.oschina.io/categories/闲扯/"},{"name":"职业思考","slug":"闲扯/职业思考","permalink":"http://zeco.oschina.io/categories/闲扯/职业思考/"}],"tags":[{"name":"about futrue","slug":"about-futrue","permalink":"http://zeco.oschina.io/tagcloud/about-futrue/"}]},{"title":"PHP 性能分析与实验：性能的微观分析[转]","slug":"php_2","date":"un11fin11","updated":"un44fin44","comments":true,"path":"2017/04/17/php_2/","link":"","permalink":"http://zeco.oschina.io/2017/04/17/php_2/","excerpt":"在上一篇文章中，我们从 PHP 是解释性语言、动态语言和底层实现等三个方面，探讨了 PHP 性能的问题。本文就深入到 PHP 的微观层面，我们来了解 PHP 在使用和编写代码过程中，性能方面，可能需要注意和提升的地方。 在开始分析之前，我们得掌握一些与性能分析相关的函数。这些函数让我们对程序性能有更好的分析和评测。","text":"在上一篇文章中，我们从 PHP 是解释性语言、动态语言和底层实现等三个方面，探讨了 PHP 性能的问题。本文就深入到 PHP 的微观层面，我们来了解 PHP 在使用和编写代码过程中，性能方面，可能需要注意和提升的地方。 在开始分析之前，我们得掌握一些与性能分析相关的函数。这些函数让我们对程序性能有更好的分析和评测。 性能分析相关的函数与命令1.时间度量函数平时我们常用 time() 函数，但是返回的是秒数，对于某段代码的内部性能分析，到秒的精度是不够的。于是要用 microtime 函数。而 microtime 函数可以返回两种形式，一是字符串的形式，一是浮点数的形式。不过需要注意的是，在缺省的情况下，返回的精度只有4位小数。为了获得更高的精确度，我们需要配置 precision。 如下是 microtime 的使用结果:12345$start= microtime(true); echo $start.&quot;/n&quot;; $end = microtime(true); echo $end.&quot;/n&quot;; echo ($end-$start).&quot;/n&quot;; 输出为：1234bash-3.2# phptime.php 1441360050.3286 1441360050.3292 0.00053000450134277 而在代码前面加上一行：1ini_set(&quot;precision&quot;, 16); 输出为：1234bash-3.2# phptime.php 1441360210.932628 1441360210.932831 0.0002031326293945312 除了 microtime 内部统计之外， 还可以使用 getrusage 来取得用户态的时长。在实际的操作中，也常用 time 命令来计算整个程序的运行时长，通过多次运行或者修改代码后运行，得到不同的时间长度以得到效率上的区别。 具体用法是：time phptime.php ，则在程序运行完成之后，不管是否正常结束退出，都会有相关的统计。1234567bash-3.2# time phptime.php 1441360373.150756 1441360373.150959 0.0002031326293945312 real 0m0.186s user 0m0.072s sys 0m0.077s 因为本文所讨论的性能问题，往往分析上百万次调用之后的差距与趋势，为了避免代码中存在一些时间统计代码，后面我们使用 time 命令居多。 2.内存使用相关函数分析内存使用的函数有两个：memory get usage、memory get peak_usage，前者可以获得程序在调用的时间点，即当前所使用的内存，后者可以获得到目前为止高峰时期所使用的内存。所使用的内存以字节为单位。1234567 $base_memory= memory_get_usage(); echo &quot;Hello,world!/n&quot;; $end_memory= memory_get_usage(); $peak_memory= memory_get_peak_usage(); echo $base_memory,&quot;/t&quot;,$end_memory,&quot;/t&quot;,($end_memory-$base_memory),&quot;/t&quot;, $peak_memory,&quot;/n&quot;;``` 输出如下： bash-3.2# phphelloworld.php Hello,world! 224400 224568 168 227424 12可以看到，即使程序中间只输出了一句话，再加上变量存储，也消耗了168个字节的内存。对于同一程序，不同 PHP 版本对内存的使用并不相同，甚至还差别很大。 $baseMemory= memory_get_usage(); class User { private $uid; function __construct($uid) { $this-&gt;uid= $uid; } } for($i=0;$i&lt;100000;$i++) { $obj= new User($i); if ( $i% 10000 === 0 ) { echo sprintf( &apos;%6d: &apos;, $i), memory_get_usage(), &quot; bytes/n&quot;; } } echo &quot; peak: &quot;,memory_get_peak_usage(true), &quot; bytes/n&quot;; 12在 PHP 5.2 中，内存使用如下：` [root@localhostphpperf]# php52 memory.php 0: 93784 bytes 10000: 93784 bytes …… 80000: 93784 bytes 90000: 93784 bytes peak: 262144 bytes 1PHP 5.3 中，内存使用如下 [root@localhostphpperf]# phpmemory.php 0: 634992 bytes 10000: 634992 bytes …… 80000: 634992 bytes 90000: 634992 bytes peak: 786432 bytes 12可见 PHP 5.3 在内存使用上要粗放了一些。PHP 5.4 – 5.6 差不多，有所优化： [root@localhostphpperf]# php56 memory.php 0: 224944 bytes 10000: 224920 bytes …… 80000: 224920 bytes 90000: 224920 bytes peak: 262144 bytes 1而 PHP 7 在少量使用时，高峰内存的使用，增大很多。 [root@localhostphpperf]# php7 memory.php 0: 353912 bytes 10000: 353912 bytes …… 80000: 353912 bytes 90000: 353912 bytes peak: 2097152 bytes 12从上面也看到，以上所使用的 PHP 都有比较好的垃圾回收机制，10万次初始化,并没有随着对象初始化的增多而增加内存的使用。PHP7 的高峰内存使用最多，达到了接近 2M。下面再来看一个例子，在上面的代码的基础上，我们加上一行，如下： $obj-&gt;self = $obj; 1代码如下： $baseMemory= memory_get_usage(); class User { private $uid; function __construct($uid) { $this-&gt;uid= $uid; } } for($i=0;$i&lt;100000;$i++) { $obj= new User($i); $obj-&gt;self = $obj; if ( $i% 5000 === 0 ) { echo sprintf( &apos;%6d: &apos;, $i), memory_get_usage(), &quot; bytes/n&quot;; } } echo &quot; peak: &quot;,memory_get_peak_usage(true), &quot; bytes/n&quot;; 12345678910111213这时候再来看看内存的使用情况，中间表格主体部分为内存使用量，单位为字节。![](/css/images/php_2_1.jpg)图表如下：![](/css/images/php_2_2.jpg)PHP 5.2 并没有合适的垃圾回收机制，导致内存使用越来越多。而5.3 以后内存回收机制导致内存稳定在一个区间。而也可以看见 PHP7 内存使用最少。把 PHP 5.2 的图形去掉了之后，对比更为明显。![](/css/images/php_2_3.jpg)可见 PHP7 不仅是在算法效率上，有大幅度的提升，在大批量内存使用上也有大幅度的优化（尽管小程序的高峰内存比历史版本所用内存更多）。---#### 3.垃圾回收相关函数在 PHP 中，内存回收是可以控制的，我们可以显式地关闭或者打开垃圾回收，一种方法是通过修改配置，zend.enable_gc=Off 就可以关掉垃圾回收。 缺省情况下是 On 的。另外一种手段是通过 gc _enable()和gc _disable()函数分别打开和关闭垃圾回收。比如在上面的例子的基础上，我们关闭垃圾回收，就可以得到如下数据表格和图表。代码如下： gc_disable(); $baseMemory= memory_get_usage(); class User { private $uid; function __construct($uid) { $this-&gt;uid= $uid; } } for($i=0;$i&lt;100000;$i++) { $obj= new User($i); $obj-&gt;self = $obj; if ( $i% 5000 === 0 ) { echo sprintf( &apos;%6d: &apos;, $i), memory_get_usage(), &quot; bytes/n&quot;; } } echo &quot; peak: &quot;,memory_get_peak_usage(true), &quot; bytes/n&quot;; 1234567891011121314分别在 PHP 5.3、PHP5.4 、PHP5.5、PHP5.6 、PHP7 下运行，得到如下内存使用统计表。![](/css/images/php_2_4.jpg)图表如下，PHP7 还是内存使用效率最优的。![](/css/images/php_2_5.jpg)从上面的例子也可以看出来，尽管在第一个例子中，PHP7 的高峰内存使用数是最多的，但是当内存使用得多时，PHP7 的内存优化就体现出来了。这里值得一提的是垃圾回收，尽管会使内存减少，但是会导致速度降低，因为垃圾回收也是需要消耗 CPU 等其他系统资源的。Composer 项目就曾经因为在计算依赖前关闭垃圾回收，带来成倍性能提升，引发广大网友关注。详见：&lt;a href=&quot;https://github.com/composer/composer/commit/ac676f47f7bbc619678a29deae097b6b0710b799&quot;&gt;github&lt;/a&gt;在常见的代码和性能分析中，出了以上三类函数之外，还常使用的有堆栈跟踪函数、输出函数，这里不再赘述。---### PHP 性能分析10则下面我们根据小程序来验证一些常见的性能差别。#### 1.使用 echo 还是 print在有的建议规则中，会建议使用 echo ，而不使用 print。说 print 是函数，而 echo 是语法结构。实际上并不是如此，print 也是语法结构，类似的语法结构，还有多个，比如 list、isset、require 等。不过对于 PHP 7 以下 PHP 版本而言，两者确实有性能上的差别。如下两份代码： for($i=0; $i&lt;1000000; $i++) { echo(&quot;Hello,World!&quot;); } for($i=0; $i&lt;1000000; $i++) { print (&quot;Hello,World!&quot;); } 1在 PHP 5.3 中运行速度分别如下（各2次）： [root@localhostphpperf]# time php echo1.php &gt; /dev/null real 0m0.233s user 0m0.153s sys 0m0.080s [root@localhostphpperf]# time php echo1.php &gt; /dev/null real 0m0.234s user 0m0.159s sys 0m0.073s [root@localhostphpperf]# time phpecho.php&gt; /dev/null real 0m0.203s user 0m0.130s sys 0m0.072s [root@localhostphpperf]# time phpecho.php&gt; /dev/null real 0m0.203s user 0m0.128s sys 0m0.075s 1在 PHP5.3 版中效率差距10%以上。而在 PHP5.4 以上的版本中，区别不大，如下是 PHP7 中的运行效率。 [root@localhostphpperf]# time php7 echo.php&gt; /dev/null real 0m0.151s user 0m0.088s sys 0m0.062s [root@localhostphpperf]# time php7 echo.php&gt; /dev/null real 0m0.145s user 0m0.084s sys 0m0.061s [root@localhostphpperf]# time php7 echo1.php &gt; /dev/null real 0m0.140s user 0m0.075s sys 0m0.064s [root@localhostphpperf]# time php7 echo1.php &gt; /dev/null real 0m0.146s user 0m0.077s sys 0m0.069s 123456正如浏览器前端的一些优化准则一样，没有啥特别通用的原则，往往根据不同的情况和版本，规则也会存在不同。---#### 2.require 还是 require_once？在一些常规的优化规则中，会提到，建议使用 require_ once 而不是 require，现由是 require_ once 会去检测是否重复，而 require 则不需要重复检测。在大量不同文件的包含中，require_ once 略慢于 require。但是 require_ once 的检测是一项内存中的行为，也就是说即使有数个需要加载的文件，检测也只是内存中的比较。而 require 的每次重新加载，都会从文件系统中去读取分析。因而 require_ once 会比 require 更佳。咱们也使用一个例子来看一下。 str.php global$str; $str= &quot;China has a large population&quot;; require.php for($i=0; $i&lt;100000; $i++) { require &quot;str.php&quot;; } require_once.php for($i=0; $i&lt;100000; $i++) { require_once&quot;str.php&quot;; } 1上面的例子，在 PHP7 中，require_ once.php 的运行速度是 require.php 的30倍！在其他版本也能得到大致相同的结果。 [root@localhostphpperf]# time php7 require.php real 0m1.712s user 0m1.126s sys 0m0.569s [root@localhostphpperf]# time php7 require.php real 0m1.640s user 0m1.113s sys 0m0.515s [root@localhostphpperf]# time php7 require_once.php real 0m0.066s user 0m0.063s sys 0m0.003s [root@localhostphpperf]# time php7 require_once.php real 0m0.057s user 0m0.052s sys 0m0.004s 12345从上可以看到，如果存在大量的重复加载的话，require_ once 明显优于 require，因为重复的文件不再有 IO 操作。即使不是大量重复的加载，也建议使用 require_ once，因为在一个程序中，一般不会存在数以千百计的文件包含，100次内存比较的速度差距，一个文件包含就相当了。---#### 3.单引号还是双引号？单引号，还是双引号，是一个问题。一般的建议是能使用单引号的地方，就不要使用双引号，因为字符串中的单引号，不会引起解析，从而效率更高。那来看一下实际的差别。 classUser { private $uid; private $username; private $age; function __construct($uid, $username,$age){ $this-&gt;uid= $uid; $this-&gt;username = $username; $this-&gt;age = $age; } function getUserInfo() { return &quot;UID:&quot;.$this-&gt;uid.&quot; UserName:&quot;.$this-&gt;username.&quot; Age:&quot;.$this-&gt;age; } function getUserInfoSingle() { return &apos;UID:&apos;.$this-&gt;uid.&apos; UserName:&apos;.$this-&gt;username.&apos; Age&apos;.$this-&gt;age; } function getUserInfoOnce() { return &quot;UID:{$this-&gt;uid}UserName:{$this-&gt;username} Age:{$this-&gt;age}&quot;; } function getUserInfoSingle2() { return &apos;UID:{$this-&gt;uid} UserName:{$this-&gt;username} Age:{$this-&gt;age}&apos;; } } for($i=0; $i&lt;1000000;$i++) { $user = new User($i, &quot;name&quot;.$i, $i%100); $user-&gt;getUserInfoSingle(); } 123在上面的 User 类中，有四个不同的方法,完成一样的功能，就是拼接信息返回，看看这四个不同的方法的区别。##### 1.getUserInfo 使用双引号和属性相拼接 [root@localhostphpperf]# time php7 string.php real 0m0.670s user 0m0.665s sys 0m0.002s [root@localhostphpperf]# time php7 string.php real 0m0.692s user 0m0.689s sys 0m0.002s [root@localhostphpperf]# time php7 string.php real 0m0.683s user 0m0.672s sys 0m0.004s 12##### 2.getUserInfoSingle 使用单引号和属性相拼接 [root@localhostphpperf]# time php7 string.php real 0m0.686s user 0m0.683s sys 0m0.001s [root@localhostphpperf]# time php7 string.php real 0m0.671s user 0m0.666s sys 0m0.003s [root@localhostphpperf]# time php7 string.php real 0m0.669s user 0m0.666s sys 0m0.002s 123可见在拼接中，单双引号并无明显差别。##### 3.getUserInfoOnce不再使用句号.连接，而是直接引入在字符串中解析。 [root@localhostphpperf]# time php7 string.php real 0m0.564s user 0m0.556s sys 0m0.006s [root@localhostphpperf]# time php7 string.php real 0m0.592s user 0m0.587s sys 0m0.004s [root@localhostphpperf]# time php7 string.php real 0m0.563s user 0m0.559s sys 0m0.003s 123从上面可见，速度提高了0.06s-0.10s，有10%-20%的效率提升。可见连缀效率更低一些。##### 4.getUserInfoSingle2 虽然没有达到我们真正想要的效果，功能是不正确的，但是在字符串中，不再需要解析变量和获取变量值，所以效率确实有大幅度提升。 [root@localhostphpperf]# time php7 string.php real 0m0.379s user 0m0.375s sys 0m0.003s [root@localhostphpperf]# time php7 string.php real 0m0.399s user 0m0.394s sys 0m0.003s [root@localhostphpperf]# time php7 string.php real 0m0.377s user 0m0.371s sys 0m0.004s 12效率确实有了大的提升，快了50%。那么这个快，是由于不需要变量引用解析带来的，还是只要加入$天然的呢？我们再试着写了一个方法。 functiongetUserInfoSingle3() { return &quot;UID:{\\$this-&gt;uid} UserName:{\\$this-&gt;username} Age:{\\$this-&gt;age}&quot;; } 1得到如下运行时间： [root@localhostphpperf]# time php7 string.php real 0m0.385s user 0m0.381s sys 0m0.002s [root@localhostphpperf]# time php7 string.php real 0m0.382s user 0m0.380s sys 0m0.002s [root@localhostphpperf]# time php7 string.php real 0m0.386s user 0m0.380s sys 0m0.004s 12345发现转义后的字符串，效率跟单引号是一致的，从这里也可以看见，单引号还是双引号包含，如果不存在需要解析的变量，几乎没有差别。如果有需要解析的变量，你也不能光用单引号，要么使用单引号和连缀，要么使用内部插值，所以在这条规则上，不用太过纠结。---#### 4.错误应该打开还是关闭？在 PHP 中，有多种错误消息，错误消息的开启是否会带来性能上的影响呢？从直觉觉得，由于错误消息，本身会涉及到 IO 输出，无论是输出到终端或者 error_log，都是如此，所以肯定会影响性能。我们来看看这个影响有多大。 error_reporting(E_ERROR); for($i=0; $i&lt;1000000;$i++) { $str= &quot;通常，$PHP中的垃圾回收机制，仅仅在循环回收算法确实运行时会有时间消耗上的增加。但是在平常的(更小的)脚本中应根本就没有性能影响。 然而，在平常脚本中有循环回收机制运行的情况下，内存的节省将允许更多这种脚本同时运行在你的服务器上。因为总共使用的内存没达到上限。&quot;; } 1234在上面的代码中，我们涉及到一个不存在的变量，所以会报出 Notice 错误:Notice: Undefined variable: PHP 中的垃圾回收机制，仅仅在循环回收算法确实运行时会有时间消耗上的增加。但是在平常的 in xxxx/string2.php on line 10如果把 E_ ERROR 改成 E_ ALL 就能看到大量的上述错误输出。我们先执行 E_ ERROR 版，这个时候没有任何错误日志输出。得到如下数据： [root@localhostphpperf]# time php7 string2.php real 0m0.442s user 0m0.434s sys 0m0.005s [root@localhostphpperf]# time php7 string2.php real 0m0.487s user 0m0.484s sys 0m0.002s [root@localhostphpperf]# time php7 string2.php real 0m0.476s user 0m0.471s sys 0m0.003s 1再执行 E_ ALL 版，有大量的错误日志输出，我们把输出重定向到/dev/null [root@localhostphpperf]# time php7 string2.php &gt; /dev/null real 0m0.928s user 0m0.873s sys 0m0.051s [root@localhostphpperf]# time php7 string2.php &gt; /dev/null real 0m0.984s user 0m0.917s sys 0m0.064s [root@localhostphpperf]# time php7 string2.php &gt; /dev/null real 0m0.945s user 0m0.887s sys 0m0.056s 12345可见慢了将近一倍。如上可见，即使输出没有正式写入文件，错误级别打开的影响也是巨大的。在线上我们应该将错误级别调到 E_ ERROR 这个级别，同时将错误写入 error_ log，既减少了不必要的错误信息输出，又避免泄漏路径等信息，造成安全隐患。#### 5.正则表达式和普通字符串操作在字符串操作中，有一条常见的规则，即是能使用普通字符串操作方法替代的，就不要使用正则表达式来处理，用 C 语言操作 PCRE 做过正则表达式处理的童鞋应该清楚，需要先 compile，再 exec，也就是说是一个相对复杂的过程。现在就比较一下两者的差别。对于简单的分隔，我们可以使用 explode 来实现，也可以使用正则表达式，比如下面的例子： ini_set(&quot;precision&quot;, 16); function microtime_ex() { list($usec, $sec) = explode(&quot; &quot;, microtime()); return $sec+$usec; } for($i=0; $i&lt;1000000; $i++) { microtime_ex(); } 1耗时在0.93-1S之间。 [root@localhostphpperf]# time php7 pregstring.php real 0m0.941s user 0m0.931s sys 0m0.007s [root@localhostphpperf]# time php7 pregstring.php real 0m0.986s user 0m0.980s sys 0m0.004s [root@localhostphpperf]# time php7 pregstring.php real 0m1.004s user 0m0.998s sys 0m0.003s 1我们再将分隔语句替换成： list($usec, $sec) = preg_split(&quot;#\\s#&quot;, microtime()); 1得到如下数据，慢了近10-20%。 [root@localhostphpperf]# time php7 pregstring1.php real 0m1.195s user 0m1.182s sys 0m0.004s [root@localhostphpperf]# time php7 pregstring1.php real 0m1.222s user 0m1.217s sys 0m0.003s [root@localhostphpperf]# time php7 pregstring1.php real 0m1.101s user 0m1.091s sys 0m0.005s 1再将语句替换成： list($usec, $sec) = preg_split(&quot;#\\s+#&quot;, microtime()); 12即匹配一到多个空格，并没有太多的影响。除了分隔外，查找我们也来看一个例子。第一段代码： $str= &quot;China has a Large population&quot;; for($i=0; $i&lt;1000000; $i++) { if(preg_match(&quot;#l#i&quot;, $str)) { } } 1第二段代码： $str= &quot;China has a large population&quot;; for($i=0; $i&lt;1000000; $i++) { if(stripos($str, &quot;l&quot;)!==false) { } } 12这两段代码达到的效果相同，都是查找字符串中有无 l 或者 L 字符。在 PHP 7 下运行效果如下： [root@localhostphpperf]# time php7 pregstring2.php real 0m0.172s user 0m0.167s sys 0m0.003s [root@localhostphpperf]# time php7 pregstring2.php real 0m0.199s user 0m0.196s sys 0m0.002s [root@localhostphpperf]# time php7 pregstring3.php real 0m0.185s user 0m0.182s sys 0m0.003s [root@localhostphpperf]# time php7 pregstring3.php real 0m0.184s user 0m0.181s sys 0m0.003s 1两者区别不大。再看看在 PHP5.6 中的表现。 [root@localhostphpperf]# time php56 pregstring2.php real 0m0.470s user 0m0.456s sys 0m0.004s [root@localhostphpperf]# time php56 pregstring2.php real 0m0.506s user 0m0.500s sys 0m0.005s [root@localhostphpperf]# time php56 pregstring3.php real 0m0.348s user 0m0.342s sys 0m0.004s [root@localhostphpperf]# time php56 pregstring3.php real 0m0.376s user 0m0.364s sys 0m0.003s 1可见在 PHP 5.6 中表现还是非常明显的，使用正则表达式慢了20%。PHP7 难道是对已使用过的正则表达式做了缓存？我们调整一下代码如下： $str= &quot;China has a Large population&quot;; for($i=0; $i&lt;1000000; $i++) { $pattern = &quot;#&quot;.chr(ord(&apos;a&apos;)+$i%26).&quot;#i&quot;; if($ret = preg_match($pattern, $str)!==false) { } } 1这是一个动态编译的 pattern。 $str= &quot;China has a large population&quot;; for($i=0; $i&lt;1000000; $i++) { $pattern = &quot;&quot;.chr(ord(&apos;a&apos;)+$i%26).&quot;&quot;; if($ret = stripos($str, $pattern)!==false) { } } 1在 PHP7 中，得到了如下结果： [root@localhostphpperf]# time php7 pregstring2.php real 0m0.351s user 0m0.346s sys 0m0.004s [root@localhostphpperf]# time php7 pregstring2.php real 0m0.359s user 0m0.352s sys 0m0.004s [root@localhostphpperf]# time php7 pregstring3.php real 0m0.375s user 0m0.369s sys 0m0.003s [root@localhostphpperf]# time php7 pregstring3.php real 0m0.370s user 0m0.365s sys 0m0.005s 1可见两者并不明显。而在 PHP 5.6 中，同样的代码： [root@localhostphpperf]# time php56 pregstring2.php real 0m1.022s user 0m1.015s sys 0m0.005s [root@localhostphpperf]# time php56 pregstring2.php real 0m1.049s user 0m1.041s sys 0m0.005s [root@localhostphpperf]# time php56 pregstring3.php real 0m0.923s user 0m0.821s sys 0m0.002s [root@localhostphpperf]# time php56 pregstring3.php real 0m0.838s user 0m0.831s sys 0m0.004s 123456789101112在 PHP 5.6 中，stripos 版明显要快于正则表达式版，由上两例可见，PHP7对正则表达式的优化还是相当惊人的。其次也建议，能用普通字符串操作的地方，可以避免使用正则表达式。 因为在其他版本中，这个规则还是适用的。某 zend 大牛官方的分享给出如下数据：&gt;stripos(‘http://’, $website) 速度是preg_match(‘/http:\\/\\//i’, $website) 的两倍&gt;ctype_alnum()速度是preg_match(‘/^\\s*$/’)的5倍;“if ($test == (int)$test)” 比 preg_match(‘/^\\d*$/’)快5倍可以相见，正则表达式是相对低效的。---#### 6.数组元素定位查找在数组元素的查找中，有一个关键的注意点就是数组值和键的查找速度，差异非常大。了解过 PHP 扩展开发的朋友，应该清楚，数组在底层其实是 Hash 表。所以键是以快速定位的，而值却未必。下面来看例子。首先们构造一个数组： $a= array(); for($i=0;$i&lt;100000;$i++){ $a[$i] = $i; } 12在这个数组中，我们测试查找值和查找键的效率差别。第一种方法用 array_ search，第二种用 array_ key_ exists，第三种用 isset 语法结构。 代码分别如下： //查找值 foreach($a as $i) { array_search($i, $a); } //查找键 foreach($a as $i) { array_key_exists($i, $a); } //判定键是否存在 foreach($a as $i) { if(isset($a[$i])); } 1运行结果如下： [root@localhostphpperf]# time php7 array.php real 0m9.026s user 0m8.965s sys 0m0.007s [root@localhostphpperf]# time php7 array.php real 0m9.063s user 0m8.965s sys 0m0.005s [root@localhostphpperf]# time php7 array1.php real 0m0.018s user 0m0.016s sys 0m0.001s [root@localhostphpperf]# time php7 array1.php real 0m0.021s user 0m0.015s sys 0m0.004s [root@localhostphpperf]# time php7 array2.php real 0m0.020s user 0m0.014s sys 0m0.006s [root@localhostphpperf]# time php7 array2.php real 0m0.016s user 0m0.009s sys 0m0.006s 1234由上例子可见，键值查找的速度比值查找的速度有百倍以上的效率差别。因而如果能用键值定位的地方，尽量用键值定位，而不是值查找。#### 7.对象与数组在 PHP 中，数组就是字典，字典可以存储属性和属性值，而且无论是键还是值，都不要求数据类型统一，所以对象数据存储，既能用对象数据结构的属性存储数据，也能使用数组的元素存储数据。那么两者有何差别呢？使用对象： classUser { public $uid; public $username; public $age; function getUserInfo() { return &quot;UID:&quot;.$this-&gt;uid.&quot; UserName:&quot;.$this-&gt;username.&quot; Age:&quot;.$this-&gt;age; } } for($i=0; $i&lt;1000000;$i++) { $user = new User(); $user-&gt;uid= $i; $user-&gt;age = $i%100; $user-&gt;username=&quot;User&quot;.$i; $user-&gt;getUserInfo(); } 1使用数组： functiongetUserInfo($user) { return &quot;UID:&quot;.$user[&apos;uid&apos;].&quot; UserName:&quot;.$user[&apos;username&apos;].&quot; Age:&quot;.$user[&apos;age&apos;]; } for($i=0; $i&lt;1000000;$i++) { $user = array(&quot;uid&quot;=&gt;$i,&quot;age&quot; =&gt;$i%100,&quot;username&quot;=&gt;&quot;User&quot;.$i); getUserInfo($user); } 1我们分别在 PHP5.3、PHP 5.6 和 PHP 7 中运行这两段代码。 [root@localhostphpperf]# time phpobject.php real 0m2.144s user 0m2.119s sys 0m0.009s [root@localhostphpperf]# time phpobject.php real 0m2.106s user 0m2.089s sys 0m0.013s [root@localhostphpperf]# time php object1.php real 0m1.421s user 0m1.402s sys 0m0.016s [root@localhostphpperf]# time php object1.php real 0m1.431s user 0m1.410s sys 0m0.012s 1在 PHP 5.3 中，数组版比对象版快了近30%。 [root@localhostphpperf]# time php56 object.php real 0m1.323s user 0m1.319s sys 0m0.002s [root@localhostphpperf]# time php56 object.php real 0m1.414s user 0m1.400s sys 0m0.006s [root@localhostphpperf]# time php56 object1.php real 0m1.356s user 0m1.352s sys 0m0.002s [root@localhostphpperf]# time php56 object1.php real 0m1.364s user 0m1.349s sys 0m0.006s [root@localhostphpperf]# time php7 object.php real 0m0.642s user 0m0.638s sys 0m0.003s [root@localhostphpperf]# time php7 object.php real 0m0.606s user 0m0.602s sys 0m0.003s [root@localhostphpperf]# time php7 object1.php real 0m0.615s user 0m0.613s sys 0m0.000s [root@localhostphpperf]# time php7 object1.php real 0m0.615s user 0m0.611s sys 0m0.003s 123456到了 PHP 5.6 和 PHP7 中，两个版本基本没有差别，而在 PHP7 中的速度是 PHP5.6 中的2倍。在新的版本中，差别已几乎没有，那么为了清楚起见我们当然应该声明类，实例化类来存储对象数据。---#### 8.getter 和 setter从 Java 转过来学习 PHP 的朋友，在对象声明时，可能习惯使用 getter 和 setter，那么，在 PHP 中，使用 getter 和 setter 是否会带来性能上的损失呢？同样，先上例子。无 setter版： classUser { public $uid; public $username; public $age; function getUserInfo() { return &quot;UID:&quot;.$this-&gt;uid.&quot; UserName:&quot;.$this-&gt;username.&quot; Age:&quot;.$this-&gt;age; } } for($i=0; $i&lt;1000000;$i++) { $user = new User(); $user-&gt;uid= $i; $user-&gt;age = $i%100; $user-&gt;username=&quot;User&quot;.$i; $user-&gt;getUserInfo(); } 1有 setter版： classUser { public $uid; private $username; public $age; function setUserName($name) { $this-&gt;username = $name; } function getUserInfo() { return &quot;UID:&quot;.$this-&gt;uid.&quot; UserName:&quot;.$this-&gt;username.&quot; Age:&quot;.$this-&gt;age; } } for($i=0; $i&lt;1000000;$i++) { $user = new User(); $user-&gt;uid= $i; $user-&gt;age = $i%100; $user-&gt;setUserName(&quot;User&quot;.$i); $user-&gt;getUserInfo(); } 1这里只增加了一个 setter。运行结果如下： [root@localhostphpperf]# time php7 object.php real 0m0.607s user 0m0.602s sys 0m0.004s [root@localhostphpperf]# time php7 object.php real 0m0.598s user 0m0.596s sys 0m0.000s [root@localhostphpperf]# time php7 object2.php real 0m0.673s user 0m0.669s sys 0m0.003s [root@localhostphpperf]# time php7 object2.php real 0m0.668s user 0m0.664s sys 0m0.004s 123456从上面可以看到，增加了一个 setter，带来了近10%的效率损失。可见这个性能损失是相当大的，在 PHP 中，我们没有必要再来做 setter 和 getter了。需要引用的属性，直接使用即可。---#### 9.类属性该声明还是不声明PHP 本身支持属性可以在使用时增加，也就是不声明属性，可以在运行时添加属性。那么问题来了，事先声明属性与事后增加属性，是否会有性能上的差别。这里也举一个例子探讨一下。事先声明了属性的代码就是2.8节中，无 setter 的代码，不再重复。而无属性声明的代码如下： classUser { function getUserInfo() { return &quot;UID:&quot;.$this-&gt;uid.&quot; UserName:&quot;.$this-&gt;username.&quot; Age:&quot;.$this-&gt;age; } } for($i=0; $i&lt;1000000;$i++) { $user = new User(); $user-&gt;uid= $i; $user-&gt;age = $i%100; $user-&gt;username=&quot;User&quot;.$i; $user-&gt;getUserInfo(); } 1两段代码，运行结果如下： [root@localhostphpperf]# time php7 object.php real 0m0.608s user 0m0.604s sys 0m0.003s [root@localhostphpperf]# time php7 object.php real 0m0.615s user 0m0.605s sys 0m0.003s [root@localhostphpperf]# time php7 object3.php real 0m0.733s user 0m0.728s sys 0m0.004s [root@localhostphpperf]# time php7 object3.php real 0m0.727s user 0m0.720s sys 0m0.004s 123456从上面的运行可以看到，无属性声明的代码慢了20%。可以推断出来的就是对于对象的属性，如果事先知道的话，我们还是事先声明的好，这一方面是效率问题，另一方面，也有助于提高代码的可读性呢。---#### 10.图片操作 API 的效率差别在图片处理操作中，一个非常常见的操作是将图片缩放成小图。缩放成小图的办法有多种，有使用 API 的，有使用命令行的。在 PHP 中，有 iMagick 和 gmagick 两个扩展可供操作，而命令行则一般使用 convert 命令来处理。我们这里来讨论使用 imagick 扩展中的 API 处理图片的效率差别。先上代码： function imagick_resize($filename, $outname) { $thumbnail = new Imagick($filename); $thumbnail-&gt;resizeImage(200, 200, imagick::FILTER_LANCZOS, 1); $thumbnail-&gt;writeImage($outname); unset($thumbnail); } function imagick_scale($filename, $outname) { $thumbnail = new Imagick($filename); $thumbnail-&gt;scaleImage(200, 200); $thumbnail-&gt;writeImage($outname); unset($thumbnail); } function convert($func) { $cmd= &quot;find /var/data/ppt |grep jpg&quot;; $start = microtime(true); exec($cmd, $files); $index = 0; foreach($files as $key =&gt;$filename) { $outname= &quot; /tmp/$func&quot;.&quot;_&quot;.&quot;$key.jpg&quot;; $func($filename, $outname); $index++; } $end = microtime(true); echo &quot;$func $index files: &quot; . ($end- $start) . &quot;s\\n&quot;; } convert(&quot;imagick_resize&quot;); convert(&quot;imagick_scale&quot;); 1在上面的代码中，我们分别使用了 resizeImage 和 scaleImage 来进行图片的压缩，压缩的是常见的 1-3M 之间的数码相机图片，得到如下运行结果： [root@localhostphpperf]# php55 imagick.php imagick_ resize 169 files: 5.0612308979034s imagick_ scale 169 files: 3.1105840206146s [root@localhostphpperf]# php55 imagick.php imagick_ resize 169 files: 4.4953861236572s imagick_ scale 169 files: 3.1514940261841s [root@localhostphpperf]# php55 imagick.php imagick_ resize 169 files: 4.5400381088257s imagick_ scale 169 files: 3.2625908851624s ```169张图片压缩，使用 resizeImage 压缩，速度在4.5S以上，而使用 scaleImage 则在 3.2S 左右，快了将近50%，压缩的效果，用肉眼看不出明显区别。当然 resizeImage 的控制能力更强，不过对于批量处理而言，使用 scaleImage 是更好的选择，尤其对头像压缩这种频繁大量的操作。本节只是例举了图片压缩 API 作为例子，也正像 explode 和 preg_ split 一样，在 PHP 中，完成同样一件事情，往往有多种手法。建议采用效率高的做法。 以上就是关于 PHP 开发的10个方面的对比，这些点涉及到 PHP 语法、写法以及 API 的使用。有些策略随着 PHP 的发展，有的已经不再适用，有些策略则会一直有用。 有童鞋也许会说，在现实的开发应用中，上面的某些观点和解决策略，有点「然并卵」。为什么这么说呢？因为在一个程序的性能瓶颈中，最为核心的瓶颈， 往往并不在 PHP 语言本身。即使是跟 PHP 代码中暴露出来的性能瓶颈，也常在外部资源和程序的不良写法导致的瓶颈上。于是为了做好性能分析，我们需要向 PHP 的上下游戏延伸，比如延伸到后端的服务上去，比如延伸到前端的优化规则。在这两块，都有了相当多的积累和分析，雅虎也据此提出了多达35条前端优化规则， 这些同 PHP 本身的性能分析构成了一个整体，就是降低用户的访问延时。 所以前面两部分所述的性能分析，只是有助于大家了解 PHP 开发本身，写出更好的 PHP 程序，为你成为一个资深的 PHP 程序员打下基础，对于实际生产中程序的效率提升，往往帮助也不是特别显著，因为大家也看到，在文章的实例中，很多操作往往是百万次才能看出明显的性能差别。在现实的页面中，每一个请求很快执行完成，对这些基础代码的调用，往往不会有这么多次调用。不过了解这些，总是好的。 那么，对于一个程序而言，其他的性能瓶颈可能存在哪里？我们将深入探讨。所以在本系列的下两篇，我们将探讨 PHP 程序的外围效源的效率问题和前端效率问题，敬请期待。","categories":[{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/categories/php/"},{"name":"底层","slug":"php/底层","permalink":"http://zeco.oschina.io/categories/php/底层/"}],"tags":[{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/tagcloud/php/"},{"name":"底层性能","slug":"底层性能","permalink":"http://zeco.oschina.io/tagcloud/底层性能/"}]},{"title":"PHP 性能分析与实验：性能的宏观分析[转]","slug":"php_1","date":"un33fin33","updated":"un44fin44","comments":true,"path":"2017/04/05/php_1/","link":"","permalink":"http://zeco.oschina.io/2017/04/05/php_1/","excerpt":"对 PHP 性能的分析，我们从两个层面着手，把这篇文章也分成了两个部分，一个是宏观层面，所谓宏观层面，就是 PHP 语言本身和环境层面，一个是应用层面，就是语法和使用规则的层面，不过不仅探讨规则，更辅助以示例的分析。 宏观层面，也就是对 PHP 语言本身的性能分析又分为三个方面： PHP 作为解释性语言性能有其天然的缺陷 PHP 作为动态类型语言在性能上也有提升的空间 当下主流 PHP 版本本身语言引擎性能","text":"对 PHP 性能的分析，我们从两个层面着手，把这篇文章也分成了两个部分，一个是宏观层面，所谓宏观层面，就是 PHP 语言本身和环境层面，一个是应用层面，就是语法和使用规则的层面，不过不仅探讨规则，更辅助以示例的分析。 宏观层面，也就是对 PHP 语言本身的性能分析又分为三个方面： PHP 作为解释性语言性能有其天然的缺陷 PHP 作为动态类型语言在性能上也有提升的空间 当下主流 PHP 版本本身语言引擎性能 PHP 作为解释性语言的性能分析与提升PHP 作为一门脚本语言，也是解释性语言，是其天然性能受限的原因，因为同编译型语言在运行之前编译成二进制代码不同，解释性语言在每一次运行都面对原始脚本的输入、解析、编译，然后执行。如下是 PHP 作为解释性语言的执行过程。如上所示，从上图可以看到，每一次运行，都需要经历三个解析、编译、运行三个过程。 那优化的点在哪里呢？可以想见，只要代码文件确定，解析到编译这一步都是确定的，因为文件已不再变化，而执行，则由于输入参数的不同而不同。在性能优化的世界里，至上绝招就是在获得同样结果的情况下，减少操作，这就是大名鼎鼎的缓存。缓存无处不在，缓存也是性能优化的杀手锏。于是乎 OpCode 缓存这一招就出现了，只有第一次需要解析和编译，而在后面的执行中，直接由脚本到 Opcode，从而实现了性能提速。执行流程如下图所示：相对每一次解析、编译，读到脚本之后，直接从缓存读取字节码的效率会有大幅度的提升，提升幅度到底有多大呢？ 我们来做一个没有 Opcode 缓存的实验。20 个并发，总共 10000 次请求没有经过 opcode 缓存的请求，，得到如下结果：其次，我们在服务器上打开 Opcode 缓存。要想实现 opcode 缓存，只需要安装 APC、Zend OPCache、eAccelerator 扩展即可，即使安装了多个，也只启用其中一个。注意的是，修改了 php.ini 配置之后，需要重新加载 php-fpm 的配置。 这里分别启用 APC 和 Zend OPCache 做实验。启用 APC 的版本。从上面的这个实验可以看到，所用的测试页面，有 40ms 以上的时间花在了语法解析和编译这两项上。通过将这两个操作缓存，可以将这个处理过程的速度大大提升。 这里附加补充一下，OpCode 到底是什么东东，OpCode 编译之后的字节码，我们可以使用bytekit 这样的工具，或者使用 vld PHP 扩展来实现对 PHP 的代码编译。如下是 vld 插件解析代码的运行结果。可以看到每一行代码被编译成相应的 OpCode 的输出。 第二个是 PHP 语言是动态类型的语言，动态类型的语言本身由于涉及到在内存中的类型推断，比如在 PHP 中，两个整数相加，我们能得到整数值，一个整数和一个字符串相加，甚至两个字符串相加，都变成整数相加。而字符串和任何类型连接操作都成了字符串。1234567&lt;?php$a = 10.11;$b = &quot;30&quot;;var_dump($a+$b);var_dump(&quot;10&quot;+$b);var_dump(10+&quot;20&quot;);var_dump(&quot;10&quot;+&quot;20&quot;); 运行结果如下：1234float(40.11)int(40)int(30)int(30) 语言的动态类型为开发者提供了方便，语言本身则会因为动态类型而降低效率。在 Swift 中，有一个特性叫类型推断，我们可以看看类型推断会带来多大的一个效率上的差别呢？对于需要类型推断与不需要类型推断两段 Swift 代码，我们尝试编译一下看看效果如何。 第一段代码如下：这是一段 Swift 代码，字典只有 14 个键值对，这段代码的编译，9 分钟了还没有编译完成（5G 内存，2.4GHz CPU），编译环境为 Swift 1.2，Xcode 6.4。但是如果调整代码如下：也就是加上了类型限定，避免了 planeLocation 的类型推断。编译过程花了 2S 。可见，作为动态类型附加的类型推断操作极大地降低了程序的编译速度。 当然，这个例子有点极端，用 Swift 来类比 PHP 也不一定合适，因为 Swift 语言本身也还在不断的进化过程中。本例子只是表明在编程语言中，如果是动态类型语言，就涉及到对动态类型的处理，从编译的角度讲是会受影响的。 那么作为动态类型的 PHP 的效率如何提升呢？从 PHP 语言本身这个层面是没有办法解决的，因为你怎么写也是动态类型的代码。解决办法就是将PHP转化为静态类型的表示，也就是做成扩展，可以看到，鸟哥的很多项目，比如 Yaf 框架，都是做成了扩展的，当然这也是由于鸟哥是 C 高手。扩展由于是 C 或者 C++ 而写，所以不再是动态类型，又加之是编译好的，而 C 语言本身的效率也会提升很多。所以效率会大幅度提高。 下面我们来看一段代码，这段代码，只是实现了简单的素数运算，能计算指定值以内的素数个数，用的是普通的筛选法。现在看看扩展实现，跟 PHP 原生实现的效率差别，这个差别当然，不仅仅是动态类型和编译类型的差别，还有语言效率的差别。 首先是用纯 PHP 写成的算法，计算 1000 万以内的素数个数，耗时在 33s 上下，实验了三次，得到的结果基本相同。其次，我们将这个求素数个数的过程，编写成了 PHP 扩展，在扩展中实现了 getprimenumbers 函数，输入一个整数，返回小于该整数的素数。得到的结果如下，这个效率的提升是非常惊人的，在 1.4s 上下即返回。速度提升 20 倍以上。可以想见，静态和编译类型的语言，其效率得到了惊人的提升。本程序的 C 语言代码如下：12345678910111213141516171819202122232425262728293031323334353637383940PHP_FUNCTION(get_prime_numbers)&#123; long value; if (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, &quot;l&quot;, &amp;value) == FAILURE) &#123; return; &#125; int *numbers = (int *)malloc(sizeof(int)*128*10000); memset(numbers, 0x0, 128*10000); int num = 2; numbers[0] = 2; numbers[1] = 3; bool flag = true; double f = 0; int i = 0; int j = 0; for(i=5; i&lt;=value; i+=2) &#123; flag = true; f = sqrt(i); for(j=0; j&lt;num;j++) &#123; if(i%numbers[j]==0) &#123; flag = false; break; &#125; if(numbers[j]&gt;f) &#123; break; &#125; &#125; if(flag) &#123; numbers[num] = i; num++; &#125; &#125; free(numbers); RETURN_LONG(num);&#125; PHP 语言本身底层性能引擎提升第三个性能优化层面是语言本身的性能提升，这个就不是我们普通开发者所能做的了。在 PHP 7以前，寄希望于小版本的改进，但是改进幅度不是非常的显著，比如 PHP 5.3 、PHP 5.4、PHP 5.5、PHP 5.5 对同一段代码的性能比较，有一定程度的进步。 PHP 5.3 的版本在上面的例子中已讲过，需要 33s 左右的时间，我们现在来看别的PHP版本。分别运行如下： PHP 5.4 版，相较 5.3 版已经有一定程度的提升。快 6 秒左右。PHP 5.5 版在 PHP 5.4的基础上又进了一步，快了 6S。PHP5.6 反而有些退步。PHP 7 果真是效率提升惊人，是 PHP5.3 的 3 倍以上。以上是求素数脚本在各个 PHP 版本之间的运行速度区别，尽管只测试了这一个程序，也不是特别的严谨，但是这是在同一台机器上，而且编译 configure 参数也基本一样，还是有一定可比性的。 在宏观层面，除了上面的这些之外，在实际的部署过程中，对 PHP 性能的优化，还体现为要减少在运行中所消耗的资源。所以 FastCGI 模式和 mod_php 的模式比传统的 CGI 模式也更为受欢迎。因为在传统的 CGI 模式中，在每一次脚本运行都需要加载所有的模块。而在程序运行完成了之后，也要释放模块资源。如下图所示：而在 FastCGI 和 mod_php 模式中，则不需要如此。只有 php-fpm 或者 Apache 启动的时候，需要加载一次所有的模块，在具体的某次运行过程中，并不需要再次加载和释放相关的模块资源。这样程序性能的效率得到了提升。以上就是有关 PHP 宏观层面的性能优化的分析，在本文的第二部分我们将探讨应用方面的 PHP 优化准则。敬请期待！","categories":[{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/categories/php/"},{"name":"底层","slug":"php/底层","permalink":"http://zeco.oschina.io/categories/php/底层/"}],"tags":[{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/tagcloud/php/"},{"name":"底层性能","slug":"底层性能","permalink":"http://zeco.oschina.io/tagcloud/底层性能/"}]},{"title":"网站设计中的cache技术","slug":"cache_1","date":"un44fin44","updated":"un00fin00","comments":true,"path":"2017/03/02/cache_1/","link":"","permalink":"http://zeco.oschina.io/2017/03/02/cache_1/","excerpt":"页面缓存的原理大体是在浏览器对资源的第一次请求之后，把资源中的一部分存储在计算机的临时文件空间，再次请求的时候，按照特定的策略加载缓存的资源，减少HTTP请求次数与传输数据量，以此提高浏览效率，减少数据库和服务器的压力。 在网站设计时要充分考虑缓存，合理的利用缓存和静态技术可以大大的提高网站的运行速度。","text":"页面缓存的原理大体是在浏览器对资源的第一次请求之后，把资源中的一部分存储在计算机的临时文件空间，再次请求的时候，按照特定的策略加载缓存的资源，减少HTTP请求次数与传输数据量，以此提高浏览效率，减少数据库和服务器的压力。 在网站设计时要充分考虑缓存，合理的利用缓存和静态技术可以大大的提高网站的运行速度。 页面静态化将动态的页面生成静态的页面保存下来，但用户访问特定的页面时，直接将缓存好的静态页面返回给客户，省去了去数据库读取数据的过程，大大提高了网站的运行效率，降低了数据库的压力。 这里牺牲了网站数据的及时性，一般静态化缓存会和其他技术结合起来使用，而不是单独使用。 php里一般使用ob方法来实现页面静态缓存123ob_start();$content = ob_get_contents();ob_end_clean(); 页面部分静态化使用模块化设计，将不需要数据动态化的部分进行静态化，使用ob函数输出，也可以使用其他技术，如ESI。 一般即使动态的数据，如果数据变化不是十分频繁（我们通过设计固定时间来更新我们的数据），也可以通过一些标识（如：id）来保存静态化数据的页面，然后通过（id）来找到静态页面位置，当该页面被访问时，直接返回给用户。 这里需要设计人员分割是否需要动态数据显示 memcached（redis）缓存内存缓存，一般使用memcached或者redis来实现，将数据以key&amp;value的方式保存在内存中，因为数据缓存在内存中，无需再去数据库里读取，这样不仅大大减轻了数据库的压力，也一样提升网站的处理速度。 这里要注意memcached和redis的选用，如果只是单纯的key&amp;value的数据读取，那么memecached是不错的选择，如果对数据安全有要求，又需要比较复杂的数据存储形式，那么redis可能会更加适合你。具体详见Memcached和Redis 关于缓存缓存有时候是让人讨厌的东西，但对于网页设计确实不得不去考虑的东西，这里简单的列举了三个比较常见的缓存技术，当然还有许多其他的缓存技术，留待以后慢慢的整理。","categories":[{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/categories/php/"},{"name":"静态技术","slug":"php/静态技术","permalink":"http://zeco.oschina.io/categories/php/静态技术/"}],"tags":[{"name":"cache","slug":"cache","permalink":"http://zeco.oschina.io/tagcloud/cache/"},{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/tagcloud/php/"}]},{"title":"使用phpspider开发的PHP爬虫","slug":"phpspider_1","date":"un22fin22","updated":"un66fin66","comments":true,"path":"2017/01/17/phpspider_1/","link":"","permalink":"http://zeco.oschina.io/2017/01/17/phpspider_1/","excerpt":"开源库出处：Github 开发框架phpspider是一个爬虫开发框架。使用本框架，你不用了解爬虫的底层技术实现，爬虫被网站屏蔽、有些网站需要登录或验证码识别才能爬取等问题。简单几行PHP代码，就可以创建自己的爬虫，利用框架封装的多进程Worker类库，代码更简洁，执行效率更高速度更快。","text":"开源库出处：Github 开发框架phpspider是一个爬虫开发框架。使用本框架，你不用了解爬虫的底层技术实现，爬虫被网站屏蔽、有些网站需要登录或验证码识别才能爬取等问题。简单几行PHP代码，就可以创建自己的爬虫，利用框架封装的多进程Worker类库，代码更简洁，执行效率更高速度更快。demo目录下有一些特定网站的爬取规则，只要你安装了PHP环境，代码就可以在命令行下直接跑。 糗事百科案例1234567891011121314151617181920212223242526272829303132$configs = array( &apos;name&apos; =&gt; &apos;糗事百科&apos;, &apos;domains&apos; =&gt; array( &apos;qiushibaike.com&apos;, &apos;www.qiushibaike.com&apos; ), &apos;scan_urls&apos; =&gt; array( &apos;http://www.qiushibaike.com/&apos; ), &apos;content_url_regexes&apos; =&gt; array( &quot;http://www.qiushibaike.com/article/\\d+&quot; ), &apos;list_url_regexes&apos; =&gt; array( &quot;http://www.qiushibaike.com/8hr/page/\\d+\\?s=\\d+&quot; ), &apos;fields&apos; =&gt; array( array( // 抽取内容页的文章内容 &apos;name&apos; =&gt; &quot;article_content&quot;, &apos;selector&apos; =&gt; &quot;//*[@id=&apos;single-next-link&apos;]&quot;, &apos;required&apos; =&gt; true ), array( // 抽取内容页的文章作者 &apos;name&apos; =&gt; &quot;article_author&quot;, &apos;selector&apos; =&gt; &quot;//div[contains(@class,&apos;author&apos;)]//h2&quot;, &apos;required&apos; =&gt; true ), ), ); $spider = new phpspider($configs); $spider-&gt;start(); 爬虫程序设计因为爬取的页面需要登录才能获取到关注者页面，所以从chrome登录之后把cookie拷贝下来给curl程序模拟登录。 使用两大独立循环进程组(用户索引进程组、用户详情进程组)，用的是php的pcntl扩展，封装了一个非常好用的类，使用起来和golang的携程也差不多了。 用户索引进程组先以一个用户为起点，抓取这个用户的关注了和关注者，然后合并入库，因为是多进程，所以当有两个进程在处理同一个用户入库的时候就会出现重复的用户，所以数据库用户名字段一定要建立唯一索引，当然也可以用redis这些第三方缓存来保证原子性，这个就见仁见智了。 用户详情进程组按照时间正序，拿到最先入库的用户抓取详情，并且把更新时间更新为当前时间，这样就可以变成一个死循环，程序可以无休止的跑，不断的循环更新用户信息。 程序运行过程中出现的错误,因为网站会给数据强制gzip压缩，需要通过解压来获取有效数据 $content = substr($content, 10);$content = gzinflate($content));curl_setopt( self::$ch, CURLOPT_ENCODING, ‘gzip’ ); 获取数据的作用（感想）将一堆数据进行分类分析，通过数据的分布可以分析出平常不易看出来的规律，对于我们的决策有很大的指引意义。 原文出处：event poll","categories":[{"name":"数据技术","slug":"数据技术","permalink":"http://zeco.oschina.io/categories/数据技术/"},{"name":"爬虫","slug":"数据技术/爬虫","permalink":"http://zeco.oschina.io/categories/数据技术/爬虫/"}],"tags":[{"name":"拿来的东西","slug":"拿来的东西","permalink":"http://zeco.oschina.io/tagcloud/拿来的东西/"},{"name":"爬虫","slug":"爬虫","permalink":"http://zeco.oschina.io/tagcloud/爬虫/"}]},{"title":"redis vs memecached","slug":"redis_vs_mem","date":"un66fin66","updated":"un11fin11","comments":true,"path":"2016/12/03/redis_vs_mem/","link":"","permalink":"http://zeco.oschina.io/2016/12/03/redis_vs_mem/","excerpt":"Redis 是由来自意大利西西里岛的Salvatore Sanfilippo开发的一款开源的，使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库。 这里不得不说一个勤奋的作者对一款开源软件是一件很重要的事，时至今日，与前辈memcached相比，Redis在大多数方面已经完成了全面的超越。 下面摘录一段在stackoverflow上看到关于redis的文章。","text":"Redis 是由来自意大利西西里岛的Salvatore Sanfilippo开发的一款开源的，使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库。 这里不得不说一个勤奋的作者对一款开源软件是一件很重要的事，时至今日，与前辈memcached相比，Redis在大多数方面已经完成了全面的超越。 下面摘录一段在stackoverflow上看到关于redis的文章。 Redis is more powerful, more popular, and better supported than memcached. Memcached can only do a small fraction of the things Redis can do. Redis is better even where their features overlap.For anything new, use Redis. Redis拥有着更加强大和丰富的功能 Memcached vs Redis: Direct ComparisonBoth tools are powerful, fast, in-memory data stores that are useful as a cache. Both can help speed up your application by caching database results, HTML fragments, or anything else that might be expensive to generate. 两者都是通过使用内存缓存，来帮助你的网站提速 Points to Consider1. Read/write speed: Both are extremely fast. Benchmarks vary by workload, versions, and many other factors but generally show redis to be as fast or almost as fast as memcached. I recommend redis, but not because memcached is slow. It&apos;s not. 2. Memory usage: Redis is better. memcached: You specify the cache size and as you insert items the daemon quickly grows to a little more than this size. There is never really a way to reclaim any of that space, short of restarting memcached. All your keys could be expired, you could flush the database, and it would still use the full chunk of RAM you configured it with. redis: Setting a max size is up to you. Redis will never use more than it has to and will give you back memory it is no longer using. I stored 100,000 ~2KB strings (~200MB) of random sentences into both. Memcached RAM usage grew to ~225MB. Redis RAM usage grew to ~228MB. After flushing both, redis dropped to ~29MB and memcached stayed at ~225MB. They are similarly efficient in how they store data, but only one is capable of reclaiming it. 3. Disk I/O dumping: A clear win for redis since it does this by default and has very configurable persistence. Memcached has no mechanisms for dumping to disk without 3rd party tools. 4. Scaling: Both give you tons of headroom before you need more than a single instance as a cache. Redis includes tools to help you go beyond that while memcached does not. 我们需要从以下4点来分析使用redis的优势 读写速度 内存使用 磁盘I/O dumping 缩放 memcachedMemcached is a simple volatile cache server. It allows you to store key/value pairs where the value is limited to being a string up to 1MB. It’s good at this, but that’s all it does. You can access those values by their key at extremely high speed, often saturating available network or even memory bandwidth. When you restart memcached your data is gone. This is fine for a cache. You shouldn’t store anything important there. If you need high performance or high availability there are 3rd party tools, products, and services available. redisRedis can do the same jobs as memcached can, and can do them better. Redis can act as a cache as well. It can store key/value pairs too. In redis they can even be up to 512MB. You can turn off persistence and it will happily lose your data on restart too. If you want your cache to survive restarts it lets you do that as well. In fact, that’s the default. It’s super fast too, often limited by network or memory bandwidth. If one instance of redis/memcached isn’t enough performance for your workload, redis is the clear choice. Redis includes cluster support and comes with high availability tools (redis-sentinel) right “in the box”. Over the past few years redis has also emerged as the clear leader in 3rd party tooling. Companies like Redis Labs, Amazon, and others offer many useful redis tools and services. The ecosystem around redis is much larger. The number of large scale deployments is now likely greater than for memcached. 从支持数据大小，速度，以及第三方支持来看两者的区别 The Redis SupersetRedis is more than a cache. It is an in-memory data structure server. Below you will find a quick overview of things Redis can do beyond being a simple key/value cache like memcached. Most of redis’ features are things memcached cannot do. Redis不仅仅作为一个cache来使用，他还是一个能够永久保存数据的NoSQL数据库 DocumentationRedis is better documented than memcached. While this can be subjective, it seems to be more and more true all the time. redis.io is a fantastic easily navigated resource. It lets you try redis in the browser and even gives you live interactive examples with each command in the docs. There are now 2x as many stackoverflow results for redis as memcached. 2x as many Google results. More readily accessible examples in more languages. More active development. More active client development. These measurements might not mean much individually, but in combination they paint a clear picture that support and documentation for redis is greater and much more up-to-date. Redis有着更丰富的文档支持 PersistenceBy default redis persists your data to disk using a mechanism called snapshotting. If you have enough RAM available it’s able to write all of your data to disk with almost no performance degradation. It’s almost free! In snapshot mode there is a chance that a sudden crash could result in a small amount of lost data. If you absolutely need to make sure no data is ever lost, don’t worry, redis has your back there too with AOF (Append Only File) mode. In this persistence mode data can be synced to disk as it is written. This can reduce maximum write throughput to however fast your disk can write, but should still be quite fast. There are many configuration options to fine tune persistence if you need, but the defaults are very sensible. These options make it easy to setup redis as a safe, redundant place to store data. It is a real database. Many Data TypesMemcached is limited to strings, but Redis is a data structure server that can serve up many different data types. It also provides the commands you need to make the most of those data types. Strings (commands)Simple text or binary values that can be up to 512MB in size. This is the only data type redis and memcached share, though memcached strings are limited to 1MB. Redis gives you more tools for leveraging this datatype by offering commands for bitwise operations, bit-level manipulation, floating point increment/decrement support, range queries, and multi-key operations. Memcached doesn’t support any of that. Strings are useful for all sorts of use cases, which is why memcached is fairly useful with this data type alone. Hashes (commands)Hashes are sort of like a key value store within a key value store. They map between string fields and string values. Field-&gt;value maps using a hash are slightly more space efficient than key-&gt;value maps using regular strings. Hashes are useful as a namespace, or when you want to logically group many keys. With a hash you can grab all the members efficiently, expire all the members together, delete all the members together, etc. Great for any use case where you have several key/value pairs that need to grouped. One example use of a hash is for storing user profiles between applications. A redis hash stored with the user ID as the key will allow you to store as many bits of data about a user as needed while keeping them stored under a single key. The advantage of using a hash instead of serializing the profile into a string is that you can have different applications read/write different fields within the user profile without having to worry about one app overriding changes made by others (which can happen if you serialize stale data). Lists (commands)Redis lists are ordered collections of strings. They are optimized for inserting, reading, or removing values from the top or bottom (aka: left or right) of the list. Redis provides many commands for leveraging lists, including commands to push/pop items, push/pop between lists, truncate lists, perform range queries, etc. Lists make great durable, atomic, queues. These work great for job queues, logs, buffers, and many other use cases. Sets (commands)Sets are unordered collections of unique values. They are optimized to let you quickly check if a value is in the set, quickly add/remove values, and to measure overlap with other sets. These are great for things like access control lists, unique visitor trackers, and many other things. Most programming languages have something similar (usually called a Set). This is like that, only distributed. Redis provides several commands to manage sets. Obvious ones like adding, removing, and checking the set are present. So are less obvious commands like popping/reading a random item and commands for performing unions and intersections with other sets. Sorted Sets (commands)Sorted Sets are also collections of unique values. These ones, as the name implies, are ordered. They are ordered by a score, then lexicographically. This data type is optimized for quick lookups by score. Getting the highest, lowest, or any range of values in between is extremely fast. If you add users to a sorted set along with their high score, you have yourself a perfect leader-board. As new high scores come in, just add them to the set again with their high score and it will re-order your leader-board. Also great for keeping track of the last time users visited and who is active in your application. Storing values with the same score causes them to be ordered lexicographically (think alphabetically). This can be useful for things like auto-complete features. Many of the sorted set commands are similar to commands for sets, sometimes with an additional score parameter. Also included are commands for managing scores and querying by score. Redis丰富的数据类型支持 Geo Redis has several commands for storing, retrieving, and measuring geographic data. This includes radius queries and measuring distances between points. Technically geographic data in redis is stored within sorted sets, so this isn’t a truly separate data type. It is more of an extension on top of sorted sets. Bitmap and HyperLogLogLike geo, these aren’t completely separate data types. These are commands that allow you to treat string data as if it’s either a bitmap or a hyperloglog. Bitmaps are what the bit-level operators I referenced under Strings are for. This data type was the basic building block for reddit’s recent collaborative art project: r/Place. HyperLogLog allows you to use a constant extremely small amount of space to count almost unlimited unique values with shocking accuracy. Using only ~16KB you could efficiently count the number of unique visitors to your site, even if that number is in the millions. Transactions and AtomicityCommands in redis are atomic, meaning you can be sure that as soon as you write a value to redis that value is visible to all clients connected to redis. There is no wait for that value to propagate. Technically memcached is atomic as well, but with redis adding all this functionality beyond memcached it is worth noting and somewhat impressive that all these additional data types and features are also atomic. While not quite the same as transactions in relational databases, redis also has transactions that use “optimistic locking” (WATCH/MULTI/EXEC). PipeliningRedis provides a feature called ‘pipelining’. If you have many redis commands you want to execute you can use pipelining to send them to redis all-at-once instead of one-at-a-time. Normally when you execute a command to either redis or memcached, each command is a separate request/response cycle. With pipelining, redis can buffer several commands and execute them all at once, responding with all of the responses to all of your commands in a single reply. This can allow you to achieve even greater throughput on bulk importing or other actions that involve lots of commands. Pub/SubRedis has commands dedicated to pub/sub functionality, allowing redis to act as a high speed message broadcaster. This allows a single client to publish messages to many other clients connected to a channel. Redis does pub/sub as well as almost any tool. Dedicated message brokers like RabbitMQ may have advantages in certain areas, but the fact that the same server can also give you persistent durable queues and other data structures your pub/sub workloads likely need, Redis will often prove to be the best and most simple tool for the job. Lua ScriptingYou can kind of think of lua scripts like redis’s own SQL or stored procedures. It’s both more and less than that, but the analogy mostly works. Maybe you have complex calculations you want redis to perform. Maybe you can’t afford to have your transactions roll back and need guarantees every step of a complex process will happen atomically. These problems and many more can be solved with lua scripting. The entire script is executed atomically, so if you can fit your logic into a lua script you can often avoid messing with optimistic locking transactions. ScalingAs mentioned above, redis includes built in support for clustering and is bundled with its own high availability tool called redis-sentinel. ConclusionWithout hesitation I would recommend redis over memcached for any new projects, or existing projects that don’t already use memcached. The above may sound like I don’t like memcached. On the contrary: it is a powerful, simple, stable, mature, and hardened tool. There are even some use cases where it’s a little faster than redis. I love memcached. I just don’t think it makes much sense for future development. Redis does everything memcached does, often better. Any performance advantage for memcached is minor and workload specific. There are also workloads for which redis will be faster, and many more workloads that redis can do which memcached simply can’t. The tiny performance differences seem minor in the face of the giant gulf in functionality and the fact that both tools are so fast and efficient they may very well be the last piece of your infrastructure you’ll ever have to worry about scaling. There is only one scenario where memcached makes more sense: where memcached is already in use as a cache. If you are already caching with memcached then keep using it, if it meets your needs. It is likely not worth the effort to move to redis and if you are going to use redis just for caching it may not offer enough benefit to be worth your time. If memcached isn’t meeting your needs, then you should probably move to redis. This is true whether you need to scale beyond memcached or you need additional functionality. 所感翻译无能啊，英文水平略显不足，还有就是专业纵深也不够，一些名词直接看不懂是什么意思，文章看到后半段的时候，脑子一片空白，虽然看得懂小半的意思，但是却不能理解具体的区别作用。 还有文章作者最后的一句话，I love memcached. I just don’t think it makes much sense for future development.莫名的让人有些难言的干涩，也摘下来共勉吧。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"nosql","slug":"数据库/nosql","permalink":"http://zeco.oschina.io/categories/数据库/nosql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://zeco.oschina.io/tagcloud/redis/"},{"name":"nosql","slug":"nosql","permalink":"http://zeco.oschina.io/tagcloud/nosql/"},{"name":"memcached","slug":"memcached","permalink":"http://zeco.oschina.io/tagcloud/memcached/"}]},{"title":"swoole的进程模型架构[转]","slug":"swoole_1","date":"un66fin66","updated":"un44fin44","comments":true,"path":"2016/08/13/swoole_1/","link":"","permalink":"http://zeco.oschina.io/2016/08/13/swoole_1/","excerpt":"原文出处：Rango 韩天峰 swoole的强大之处就在与其进程模型的设计，既解决了异步问题，又解决了并行。","text":"原文出处：Rango 韩天峰 swoole的强大之处就在与其进程模型的设计，既解决了异步问题，又解决了并行。 主线程MainReactorswoole启动后主线程会负责监听server socket，如果有新的连接accept，主线程会评估每个Reactor线程的连接数量。将此连接分配给连接数最少的reactor线程。这样的好处是 每个reactor线程持有的连接数是非常均衡的，没有单个线程负载过高的问题 解决了惊群问题，尤其是拥有多个listen socket时，节约了线程唤醒和切换的开销 主线程内还接管了所有信号signal的处理，使Reactor线程运行中可以不被信号打断。 管理进程Managerswoole运行中会创建一个单独的管理进程，所有的worker进程和task进程都是从管理进程Fork出来的。管理进程会监视所有子进程的退出事件，当worker进程发生致命错误或者运行生命周期结束时，管理进程会回收此进程，并创建新的进程。 管理进程还可以平滑地重启所有worker进程，以实现程序代码的重新加载。 异步Reactor线程swoole拥有多线程Reactor，所以可以充分利用多核，开启CPU亲和设置后，Reactor线程还可以绑定单独的核，节约CPU Cache开销。 swoole的Reactor线程是全异步非阻塞的，即使你的worker进程用了同步模式，依然不影响reactor线程的性能。在worker进程组很繁忙的状况下，reactor线程完全不受影响，依然可以收发处理数据。 TCP是流式的，没有边界，所以处理起来很麻烦。Reactor线程可以根据EOF或者包头长度，自动缓存数据，组装数据包。等一个请求完全收到后，再投递给Worker进程。 同步或异步Worker进程与传统的半同步半异步服务器不同，Swoole的worker进程可以是同步的也可以异步的，这样带来了最大的灵活性。当你的Server需要很高性能，业务逻辑较为简单时你可以选择异步模式。当业务逻辑复杂多变，可以选择同步模式。 这里要比Node.js强大太多了。 TaskWorker进程池swoole除了Reactor线程，Worker进程外还提供了TaskWorker进程池，目的是为了解决在业务代码中，有些逻辑部分不需要马上执行。利用task进程池，可以方便的投递一个异步任务去执行，在Worker进程空闲时再去捕获任务执行的结果。","categories":[{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/categories/php/"},{"name":"swoole","slug":"php/swoole","permalink":"http://zeco.oschina.io/categories/php/swoole/"}],"tags":[{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/tagcloud/php/"},{"name":"swoole，网络通信","slug":"swoole，网络通信","permalink":"http://zeco.oschina.io/tagcloud/swoole，网络通信/"}]},{"title":"分布式服务器集群架构方案思考","slug":"zfenbu_1","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2016/07/15/zfenbu_1/","link":"","permalink":"http://zeco.oschina.io/2016/07/15/zfenbu_1/","excerpt":"网上看到一个分布式的服务器集群架构，看了下觉得自己对于分布式的理解清晰了不少，于是摘录下来以免丢失。","text":"网上看到一个分布式的服务器集群架构，看了下觉得自己对于分布式的理解清晰了不少，于是摘录下来以免丢失。原文出处：分布式服务器集群架构方案思考 by 夏日小草 大型网站演化简单说，分布式是以缩短单个任务的执行时间来提升效率的，而集群则是通过提高单位时间内执行的任务数来提升效率。 集群主要分为：高可用集群(High Availability Cluster)，负载均衡集群(Load Balance Cluster，nginx即可实现)，科学计算集群(High Performance Computing Cluster)。 分布式是指将不同的业务分布在不同的地方；而集群指的是将几台服务器集中在一起，实现同一业务。分布式中的每一个节点，都可以做集群。 而集群并不一定就是分布式的。 之前在网上看到一篇关于大型网站演化的博客。page 每个大型网站都会有不同的架构模式，而架构内容也就是在处理均衡负载，缓存，数据库，文件系统等，只是在不同的环境下，不同的条件下，架构的模型不一样，目的旨在提高网站的性能。 最初的架构只有应用程序，数据库，文件服务。到后来，分布式服务、集群架设。 关于均衡负载方案在上一篇，《Nginx反向代理实现均衡负载》讨论过过的nginx现实均衡负载方案，这里选择另一种HAProxy+Keepalived双机高可用均衡负载方案。 HAProxy是免费、极速且可靠的用于为TCP和基于HTTP应用程序提供高可用、负载均衡和代理服务的解决方案，尤其适用于高负载且需要持久连接或7层处理机制的web站点。 不论是Haproxy还是Keepalived甚至是上游服务器均提高生产力并增强可用性,也就是如下架构中Haproxy,Keepalived,Httpd服务器任意宕机一台服务还是可以正常运行的。 HAProxy的优点： 1、HAProxy是支持虚拟主机的，可以工作在4、7层(支持多网段)； 2、能够补充Nginx的一些缺点比如Session的保持，Cookie的引导等工作； 3、支持url检测后端的服务器； 4、本身仅仅就只是一款负载均衡软件；单纯从效率上来讲HAProxy更会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的； 5、HAProxy可以对Mysql读进行负载均衡，对后端的MySQL节点进行检测和负载均衡； 关于Redis缓存方案缓存分为服务器缓存和应用程序缓存。 关于应用程序内缓存，已经在Jue后台框架里面做了模块处理了。 关于服务器缓存，主要缓存服务器文件，减少服务器和php交互，减少均衡负载服务器和应用程序服务器交互。 缓存里面有一种典型的memcached，现在用的多的是redis轻量级缓存方案。 关于memcached与redis，看这篇 《Memcached vs Redis?》 Redis主要将数据存储在各种格式：列表，数组，集合和排序集，一次能接受多个命令，阻塞读写，等待直到另一个进程将数据写入高速缓存。一篇关于Reids缓存方案。《高可用、开源的Redis缓存集群方案》 关于NoSQL快速存储方案NoSQL在这里的使用价值是处理一些琐事，比如用户个人网站的一些css值，height,width,color等等的小而繁多的数据，采用NoSQL旨在提升数据库速度，减少对MySQL的SELECT请求。 关于NoSQL的方案很多了，选一个简单的MongDB好了。 关于分布式MySQL方案(做分布式MySQL还没尝试过，初期也不清楚mysql所需要的压力，所以第一期不打算做分布式MySQL) 《标准MySQL数据库外的5个开源兼容方案》 分布式集群方案综合起来，大致就是如下模型，初探分布式架构，还有很多要修改的，待续，时时更新中… 观后所感一个完整的架构设计并不是一件简单的事，其中涉及的知识很多，我也心知一口气吃不成胖子，更多的问题留待自己有更加成熟的思考和理解之后，再来整理","categories":[{"name":"分布式部署","slug":"分布式部署","permalink":"http://zeco.oschina.io/categories/分布式部署/"}],"tags":[{"name":"拿来的东西","slug":"拿来的东西","permalink":"http://zeco.oschina.io/tagcloud/拿来的东西/"},{"name":"分布式","slug":"分布式","permalink":"http://zeco.oschina.io/tagcloud/分布式/"}]},{"title":"四种基本的排序算法学习总结","slug":"algorithm_1","date":"un44fin44","updated":"un55fin55","comments":true,"path":"2016/06/23/algorithm_1/","link":"","permalink":"http://zeco.oschina.io/2016/06/23/algorithm_1/","excerpt":"都说算法是程序的核心,但是作为一个半路出家的程序员，情不自禁的有点畏惧这个东西，但是今天却被教训了一顿，因为亲眼目睹前辈以一个简单的排序方法提升了好几倍的查询速度，好吧，决心一点点的啃下算法的硬骨头，提升一点自己coding的视野和高度。","text":"都说算法是程序的核心,但是作为一个半路出家的程序员，情不自禁的有点畏惧这个东西，但是今天却被教训了一顿，因为亲眼目睹前辈以一个简单的排序方法提升了好几倍的查询速度，好吧，决心一点点的啃下算法的硬骨头，提升一点自己coding的视野和高度。 今天就整理一下最简单的四个排序算法：冒泡、快速、选择、插入。快速排序对一个乱序的数组，每次选择一个指定位置的元素（一般是第一个），每次scan，将被选择的元素作为基准，将乱序的数组分为大小两部分，后续递归剩下的部分，直到分割出来的数组长度不可再分。具体举例如下12345678910111213141516171819202122function quick_sort($arr) &#123; $length = count($arr); if($length &lt;= 1) &#123; //递归出口 return $arr; &#125; $base_array = $arr[0]; //选择第一个元素作为基准 //初始化两个数组,保证每次递归签数组清空 $left_array = array(); //小于基准的 $right_array = array(); //大于基准的 for($i=1; $i&lt;$length; $i++) &#123; if($base_num &gt; $arr[$i]) &#123;//放入左边数组 $left_array[] = $arr[$i]; &#125; else &#123; //放入右边 $right_array[] = $arr[$i]; &#125; &#125; //递归排序 $left_array = quick_sort($left_array); $right_array = quick_sort($right_array); //合并数组 return array_merge($left_array, array($base_num), $right_array); &#125; 冒泡排序冒泡排序基本是一个programer要学习的第一个算法，但是简单不代表他真的就low，就像递归和迭代算法一样，并没有那个算法是最优的，而是适当的环境下选择适当的算法。冒泡算法，通过两层循环，从第一个元素开始，每次对比相邻的元素，依据他们的大小和需要的排序决定他们的位置，最后获得一个重新排序过的数组。具体举例如下12345678910111213function bubbleSort($arr)&#123; $len=count($arr); for($i=1;$i&lt;$len;$i++) &#123; //第一层循环控制层数 for($k=0;$k&lt;$len-$i;$k++)&#123; //第二层循环用来控制级数 if($arr[$k]&gt;$arr[$k+1])&#123; $tmp=$arr[$k+1]; $arr[$k+1]=$arr[$k]; $arr[$k]=$tmp; &#125; &#125; &#125; return $arr; &#125; 选择排序选择排序是通过假设一个假值，然后获得这个假值的位置（每次选择最大或者最小的树），寻找到他的位置，排列出一个固定顺序的数组。具体举例如下12345678910111213141516171819202122function selectSort($arr) &#123; //双重循环完成，外层控制轮数，内层控制比较次数 $len=count($arr); for($i=0; $i&lt;$len-1; $i++) &#123; //假设最小的值的位置 $p = $i; for($j=$i+1; $j&lt;$len; $j++) &#123; //比较，发现更小的,记录下最小值的位置；并且在下次比较时采用已知的最小值进行比较。 if($arr[$p] &gt; $arr[$j]) &#123; $p = $j; &#125; &#125; //已经确定了当前的最小值的位置，保存到$p中。如果发现最小值的位置与当前假设的位置$i不同，则位置互换即可。 if($p != $i) &#123; $tmp = $arr[$p]; $arr[$p] = $arr[$i]; $arr[$i] = $tmp; &#125; &#125; //返回最终结果 return $arr; &#125; 插入排序假设一个数组是有序的，现在要把第n个数插到前面的有序数中，使得这n个数也是排好顺序的。如此反复循环，直到全部排好顺序。具体举例如下123456789101112131415161718function insertSort($arr) &#123; $len=count($arr); for($i=1, $i&lt;$len; $i++) &#123; $tmp = $arr[$i]; //内层循环控制，比较并插入 for($j=$i-1;$j&gt;=0;$j--) &#123; if($tmp &lt; $arr[$j]) &#123; //发现插入的元素要小，交换位置，将后边的元素与前面的元素互换 $arr[$j+1] = $arr[$j]; $arr[$j] = $tmp; &#125; else &#123; //如果碰到不需要移动的元素，由于是已经排序好是数组，则前面的就不需要再次比较了。 break; &#125; &#125; &#125; return $arr; &#125;","categories":[{"name":"算法学习","slug":"算法学习","permalink":"http://zeco.oschina.io/categories/算法学习/"}],"tags":[{"name":"排序","slug":"排序","permalink":"http://zeco.oschina.io/tagcloud/排序/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://zeco.oschina.io/tagcloud/Algorithm/"}]},{"title":"git操作(I) 提交到远程仓库","slug":"git_1","date":"un55fin55","updated":"un22fin22","comments":true,"path":"2016/05/27/git_1/","link":"","permalink":"http://zeco.oschina.io/2016/05/27/git_1/","excerpt":"目前市面有许多第三方git操作软件，比如我曾经用过的Source Tree，就是一款上手和使用起来都不错的工具，但时间一长，总觉得差点微妙的feel，于是又灰溜溜的决定把命令行捡回来，写个系列的使用记录。","text":"目前市面有许多第三方git操作软件，比如我曾经用过的Source Tree，就是一款上手和使用起来都不错的工具，但时间一长，总觉得差点微妙的feel，于是又灰溜溜的决定把命令行捡回来，写个系列的使用记录。 创建一个本地仓库 现在，有一个尚未开始的项目，那么进入你项目存放的目录12$ cd XXX$ git init 这样你就获得了一个仓库，并且处于当前master branch上 或者通过clone一个远程的仓库到本地12$ cd XXX$ git clone url:repositories [repo&apos;s name] 需要注意的是，这个本地文件夹（XXX）要是一个空的文件夹，否则会报错 完成文件提交 创建一个本地文件1$ vim ./hello_git.md 在文件输入你想加入的内容1~ hello git :) 保存后退出 添加文件到暂存区，提交文件123456789使用$ git status查看当前文件状态，出现未添加文件，使用$ git add ./hello_git.md再查看文件状态‘$ git status这里显示文件暂存，然后准备commit$ git commit -m &apos;备注信息&apos;再次查看工作区时，显示tree clean，说明提交成功 你也可以使用 git status -s 命令查看更为详细的状态也可以通过 git log 来查看每次commit 关联远程仓库git 属于分布式版本控制系统,每个电脑就相当于一个仓库，但如果你有一个geek的心，你可以将你的代码分享到一个远程的托管系统上，其中的代表有世界上最大的同性交友网站github，还有国内的两个不错的托管平台coding和码云。 我们可以通过remote命令将自己的代码托管在远程的代码托管平台上 查看关联的远程仓库1$ git remote -v 获得已经关联的远程仓库2 . 配置本地账户信息配置一个本地的用户信息，使得仓库中的操作是由谁做出的12$ git config --global user.name &quot;your_username&quot; $ git config --global user.email your_email@domain.com 生成ssh秘钥1$ ssh-keygen -t rsa -C &quot;your_email@domain.com&quot; 输入命令之后会提示三次确认，一直回车确定，最后提示创建成功，然后进入主文件查看生成的id_rsa和id_rsa.pub，将id_rsa.pub公钥打开，添加到远程创建的长裤中 打开公钥使用编辑器，否则可能会影响编码，导致添加失败 添加远程仓库地址12$ git remote add origin https://your_username@bitbucket.org/your_username/name_of_remote_repository.git $ git push origin master 显示push成功之后，就可以再远程仓库里看到你提交的代码了 如果使用的clone仓库，那么本地默认存在远程仓库地址，只需要添加ssh公钥认证之后就可以提交代码到远程仓库了 如果你看了这些还是不太了解提交的过程，可以参考git官方给出的操作book","categories":[{"name":"工具","slug":"工具","permalink":"http://zeco.oschina.io/categories/工具/"},{"name":"git","slug":"工具/git","permalink":"http://zeco.oschina.io/categories/工具/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://zeco.oschina.io/tagcloud/git/"}]},{"title":"好玩的ReidsCycle","slug":"rediscycle","date":"un11fin11","updated":"un11fin11","comments":true,"path":"2016/05/16/rediscycle/","link":"","permalink":"http://zeco.oschina.io/2016/05/16/rediscycle/","excerpt":"最近，突发奇想的想要设计一个延时触发的项目，上网翻腾了一把，着实发现了不少有意思的东西，这里附上感觉不错的两处：一、二","text":"最近，突发奇想的想要设计一个延时触发的项目，上网翻腾了一把，着实发现了不少有意思的东西，这里附上感觉不错的两处：一、二 比较常见的方案使用cron定时任务，但是cron设定任务最低的精度到minute,某些时候，我们需要的可能是精确到second级别的定时计划。 再者，当表单数据量巨大时，使用cron进行轮询的效率就很低。 因为，每次轮询都要扫一次库，之前执行过的记录仍会被扫描，这样的效率就会下降 使用redis实现高效的延时设计参见一中的设计，这里包含两个重要的数据结构： 环形队列，一个头尾相接数组，其包含3600个slot的环形队列（一环设计为1h==60min==3600s，cycle=n?） work_ task, 置于指定时间点的 [‘set’=&gt;[cycle,dot,work_task]] timer, 一个和环形队列对应的timer是必须的,每当timer变化设定的dot值时，环形队列的指针（index）便移动一位，同事检测当前dot上的set集，对比集中cycle，如果对应上，便执行此点上的work_ task 这里我们设定一个例子： slot(3600,dot=1s) timer=dot=1s set=[task_1=&gt;[‘cycle’=&gt;3,’dot’=&gt;667,’task’=&gt;function(){echo ‘do it’}]] 任务执行的逻辑如下：123(cycle=1,dot=667)--&gt;1 != 3 &amp;&amp; 667 == 667 continue(cycle=2,dot=667)--&gt;2 != 3 &amp;&amp; 667 == 667 continue(cycle=3,dot=667)--&gt;3 == 3 &amp;&amp; 667 == 667 do work_task--&gt;echo do it--&gt;delete work_task 从任务的逻辑不难看出， 每次指针移动时，只需要查看当前dot上是否有set如果有set集则进行判断cycle值，决定是否执行任务，无需轮询全部任务，效率提升 每个订单任务被执行之后，即刻更新任务数据，一个任务只执行一次 时效性自定义，再不影响系统性能的情况下，就能够做到足够高的精确度","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"nosql","slug":"数据库/nosql","permalink":"http://zeco.oschina.io/categories/数据库/nosql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://zeco.oschina.io/tagcloud/redis/"},{"name":"环形队列","slug":"环形队列","permalink":"http://zeco.oschina.io/tagcloud/环形队列/"},{"name":"延时设计","slug":"延时设计","permalink":"http://zeco.oschina.io/tagcloud/延时设计/"}]},{"title":"PHP+salt加密","slug":"php+salt","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2016/05/13/php+salt/","link":"","permalink":"http://zeco.oschina.io/2016/05/13/php+salt/","excerpt":"","text":"最近又听闻用户信息泄露的新闻，此前，作为it小白的时候，实在不知道怎么处理这样的变故，而泄漏的信息如果被黑客利用，通过撞库的方法就可破解大多数用户的密码，毕竟用户偏向于使用自己简单容易记住的密码，也不会多去改变他们，这时，如果我们开发人员还使用单一的加密方式，那是十分不明智的。 MD5加盐加密我们对md5()这个函数再是熟悉不过了，都使用过md5()对用户密码进行加密处理，这样做没有错，因为MD5加密不可逆，但是这样做的安全性还是很低的，因为很多网站的用户数据都是用md5进行加密处理的，而且网上也有许多人为整理出来的常用的MD5加密库，而从CSDN当时泄漏出来的用户信息来看，即便是作为与网络打交道的程序员，其中也有许多使用的是简单的密码组合，及其容易被匹配出来，再者，对于黑客而言，其破解密码手段通过撞库，简单的密码组合就大大增大了密码被破解的几率。 直接去开发一个新的算法来加密，从现实的角度是不实际的，那既然从算法的角度无法实现，那么我们就可以从入口的数据下手，将用户的密码添加一些佐料，这样即便是简单的密码，也会因为我们的加密，提升了密码组合的复杂程度，不会那么轻易的被破解，这就是所谓的加salt。 12345678910$salt = get_salt(SALT); //我们通过设置不同的SALT值来获取不同等级的加密$password = &apos;we2134sda&apos;; //用户处获取的明文密码$md5_password = md5(&apos;your_site&apos;.$password.$salt);//最终加密后密码function get_salt（$param） &#123; $salt = &apos;&apos;; for ($i = 0;$i &lt; $param; $i++) &#123; $salt .= chr(mt_rand(13,$param*12+13)); &#125; return $salt;&#125; 这里我们使用md5加随机生成的salt来增强加密后的密码安全性，然后我们记录下salt值，在用户注册的时候和密码一起生成并保存到数据库中，用户登录验证的时候再把密码和盐值一起组合验证，通过这样的手段就可以加强密码的安全性。","categories":[{"name":"安全","slug":"安全","permalink":"http://zeco.oschina.io/categories/安全/"}],"tags":[{"name":"加密","slug":"加密","permalink":"http://zeco.oschina.io/tagcloud/加密/"},{"name":"md5","slug":"md5","permalink":"http://zeco.oschina.io/tagcloud/md5/"}]},{"title":"用hexo+github搭建一个静态blog","slug":"your blog","date":"un22fin22","updated":"un00fin00","comments":true,"path":"2016/05/03/your blog/","link":"","permalink":"http://zeco.oschina.io/2016/05/03/your blog/","excerpt":"关于怎么搭建blog的教程网上有一堆，挑一个比较详细的附上：如何搭建一个独立博客——简明Github Pages与Hexo教程这里就记录一些个人探过的一些小坑，以做前车之鉴。","text":"关于怎么搭建blog的教程网上有一堆，挑一个比较详细的附上：如何搭建一个独立博客——简明Github Pages与Hexo教程这里就记录一些个人探过的一些小坑，以做前车之鉴。 配置文件使用hexo搭建blog的时候系统默认了一个theme：landscape 我使用的是这个主题：ICARUS 下载之后要在主配置文件 _config.yaml里配置1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: hexo-theme-icarus //你所下载的主题仓库名称 而如果需要调整请在主题文件夹的 _config.yaml里配置 注意两者的位置主配置文件位于hexo文件的根目录中主题配置文件的目录位于hexo-&gt;themes-&gt;(your themes)-&gt;_config.yaml(一般出事文件还有后缀example，将他去掉就好了) 部署到github使用$ hexo d 就可以将生成好的静态文件部署到github上，但部署文件之前需要在主配置文件里配置123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/yourgithubname/yourgithubname.github.io.git branch: master 注意，这里的仓库名称一定要是你的：github用户名.github.io才会被github默认为pages 但需要注意的是，有些时候这样的配置不一定能够成功，因为在hexo3里使用https:// 会影响他的部署这里要将仓库地址repo改成1repo：git@github.com:username/username.github.io.git 如果还是失败，那就要检查SSH key是否添加成功，以及git的设置是否成功 关于MarkDown的书写使用hexo new post [post title],可以生成一篇新的文章，只需要到/source/_post/下就可以找到生成的title.md文件下面是标题文件的书写123456---title: blog标题categories: [一级分类，二级分类...]tags: [标签1,标签2,标签3...]thumbnail: url of image(缩略图的路径)--- 博文正文的书写惨遭markdown语法，markdown语法兼容html的语法，相信对学习过html标签都不是一件难事。这里再提一点，如果需要只显示部分的博文可以使用1&lt;!--more--&gt; 来分割显示和不显示的文章 关于pluginhexo官网上有许多优秀的plugin，我找了一个比较有意思的标签云插件按照readme一步一步配置就可以实现动态的tagcloud了 最后上一切的起源：hexo","categories":[{"name":"blog","slug":"blog","permalink":"http://zeco.oschina.io/categories/blog/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zeco.oschina.io/tagcloud/blog/"},{"name":"hexo","slug":"hexo","permalink":"http://zeco.oschina.io/tagcloud/hexo/"},{"name":"github","slug":"github","permalink":"http://zeco.oschina.io/tagcloud/github/"}]},{"title":"MySQL (III) 字段类型选择","slug":"mysql_uh3","date":"un66fin66","updated":"un44fin44","comments":true,"path":"2016/04/30/mysql_uh3/","link":"","permalink":"http://zeco.oschina.io/2016/04/30/mysql_uh3/","excerpt":"字段是组成一张表的最基本单元，但它们却是数据表设计时不得不严密考虑的部分，因为不恰当的字段类型选择，不仅容易造成数据库空间的浪费，以及冗余的产生，且在数据量巨大以及查询频繁的字段或表，将严重影响数据的读写速度。","text":"字段是组成一张表的最基本单元，但它们却是数据表设计时不得不严密考虑的部分，因为不恰当的字段类型选择，不仅容易造成数据库空间的浪费，以及冗余的产生，且在数据量巨大以及查询频繁的字段或表，将严重影响数据的读写速度。 Int（bigint，int，mediumint，smallint，tinyint）因为计算机本身只分辨数字型的数据，所以合理的使用整形字段可以提升数据读取的速度。 整形数据由大至小，从16个字节到1个字节不等，对于不同需求的表选择不同类型的int类型作为表的primary key是一个不错的选择。 这里还要提到的一点，虽然MySQL数据库提供了datetime类型的字段，但我们还是尽量以int类型的格式来存储时间戳的方法来保存时间数据，这样可以提高读取速度和减少I/O的开销。 在一些单选或者多选的字段里，也应当尽量避免使用枚举，适当的选用tinyint这样的短int数据是更好的选择。 Float（float，double，decimal）其实在浮点类型的数据中，decimal并不是作为数字存储在数据库中的，相反他是以我们’厌恶’的字符类型存在。 在这里，我们不得不来讨论一下浮点型数的一些缺点，单/双精度的数据类型在数据库中超过一定位数之后会出现失去精度的的情况，这个位数的大小在6位小数，而我们存储一些类似价格，额度等字段时，需要一种完全精确的记录，而decimal（65，30）如此长度的记录数完全满足我们的需求，这是浮点类型数据我们应当关心的部分。 字符（char，varchar）虽然字符类型的数据在读取时并不佳，但字符型的数据却是一个数据库不可忽略的部分。 char和varchar的不同体现在多个方面 1.char类型数据最大长度（255），而varchar类型数据的最大长度为（65535） 这里需要注意的是，varchar类型的数据长度存储的字节长度，即varchar类型的数据需要考虑编码的原因 2.两者最主要体现在两者侧重的方面不同，char类型数据属于定长数据，不论存储多少长度的数据，都占据一定的空间，而varchar数据则不论在表中设计多大的长度，都依照存储的字符串长度来决定占用的空间 3.因为varchar之所节省空间，是因为varchar经过了一层数据库的算法过滤，恰恰也是这层过滤，使得varchar类型的数据在读写速度上劣于char类型的数据。 总而言之，varchar和char两者，一个为了节省空间而浪费了时间，一个为了节省时间而浪费了空间，这就需要我们在设计表格的时候，谨慎的去考虑和取舍了。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"mysql","slug":"数据库/mysql","permalink":"http://zeco.oschina.io/categories/数据库/mysql/"}],"tags":[{"name":"字段优化","slug":"字段优化","permalink":"http://zeco.oschina.io/tagcloud/字段优化/"}]},{"title":"Mysql (II) 引擎","slug":"mysql_uh2","date":"un44fin44","updated":"un44fin44","comments":true,"path":"2016/04/21/mysql_uh2/","link":"","permalink":"http://zeco.oschina.io/2016/04/21/mysql_uh2/","excerpt":"这里，只简单的讨论两种常见的引擎：MyISAM和InnoDB的区别。","text":"这里，只简单的讨论两种常见的引擎：MyISAM和InnoDB的区别。 a. MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持. b. MyISAM类型的表强调的是性能，其执行数度比InnoDB类型更快. c. InnoDB不支持fulltext类型的索引. d. InnoDB 中不保存表的具体行数，也就是说，执行select count(*) from table时，InnoDB要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可. e. 对于auto_increment类型的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中，可以和其他字段一起建立联合索引。 f. delete from table时，InnoDB不会重新建立表，而是一行一行的删除。 g. load table from master操作对InnoDB是不起作用的，解决方法是首先把InnoDB表改成MyISAM表，导入数据后再改成InnoDB表，但是对于使用的额外的InnoDB特性(例如外键)的表不适用. h. MyISAM支持表锁，InnoDB支持行锁。 B-treeInnoDB的B-tree结构 如上图所示，InnoDB的搜索树由两层结构组成，每次对InnoDB引擎的表操作时，实际上是经过了两次的处理，先通过索引找到主键的位置，再获取对应主键的记录，这样不论是写入还是读取的速度都讲收到影响。 MyISAM的B-tree结构 MyISAM的结构决定了，每次读取和写入的时候不用考虑主键的顺序重排，所以MyISAM引擎的表的读取速度较InnoDB的快","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"mysql","slug":"数据库/mysql","permalink":"http://zeco.oschina.io/categories/数据库/mysql/"}],"tags":[{"name":"数据库引擎","slug":"数据库引擎","permalink":"http://zeco.oschina.io/tagcloud/数据库引擎/"}]},{"title":"Mysql (I) 数据表设计规范","slug":"mysql_uh1","date":"un00fin00","updated":"un44fin44","comments":true,"path":"2016/04/03/mysql_uh1/","link":"","permalink":"http://zeco.oschina.io/2016/04/03/mysql_uh1/","excerpt":"三范式&nbsp;&nbsp;&nbsp;&nbsp;俗话说，好的表结构是一个项目成功的一般，而在项目中我们一般按照三范式的规范来设计我们的表格。三范式的具体要求如下： 原子性（不可再分的字段） 非主键字段必须依赖主键 消除非主键之间的传递依赖","text":"三范式&nbsp;&nbsp;&nbsp;&nbsp;俗话说，好的表结构是一个项目成功的一般，而在项目中我们一般按照三范式的规范来设计我们的表格。三范式的具体要求如下： 原子性（不可再分的字段） 非主键字段必须依赖主键 消除非主键之间的传递依赖 &nbsp;&nbsp;&nbsp;&nbsp;当然，三范式在网上存在许多不同的版本，但不论是哪个版本的三范式，其中心思想都是一样的，那就是设计出比较合理的数据表，尽可能的减少代码的冗余。 但需要注意的是，代码的冗余只能减少，而不是消灭 具体设计思路如下： id name order order_name address weather price (非设计规范表) (拆分address字段保证设计表的原子性) -&gt; id name order order_name country province city weather price (将非主键依赖的weather的字段拆分出去) -&gt; id name order order_name country province city price (消除order和order_name之间的传递依赖，将非主键之间的传递依赖消除) -&gt; id name order country province city price 逆范式&nbsp;&nbsp;&nbsp;&nbsp;实际项目中，并不是完全按三范式的规范来设计表的结构，具体的项目中，逆范式的设计有时候可以简化sql的语句，提高sql语句的执行效率，此时逆范式的设计明显更有利于我们的项目，这个情况下使用逆范式的设计将更为科学。 例如，我们现在有两张表，一张分类表（category），一张商品表（goods），这里我们项目需要查询分类id、分类名称、商品数量三个字段，具体的sql语句如下：1select c.*,count(g.goods_id) goods_num from category as c left join goods as g on c.cat_id = g.cat_id group by c.cat_id; 可以看到，这个使用jion连表查询的sql语句相对于简单的表结构并不简单，而如果我们给分类表加上一个对应的商品数量字段，这样的sql语句将大大简化，这个时候我们大可不必严守三范式的设计思路，不妨使用逆范式的方式来设计这张表。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"mysql","slug":"数据库/mysql","permalink":"http://zeco.oschina.io/categories/数据库/mysql/"}],"tags":[{"name":"表设计","slug":"表设计","permalink":"http://zeco.oschina.io/tagcloud/表设计/"}]}]}