{"meta":{"title":"Zeco's blog","subtitle":null,"description":null,"author":"Zeco","url":"http://zeco.oschina.io"},"pages":[{"title":"关于作者","date":"un00fin00","updated":"un00fin00","comments":true,"path":"about/index.html","permalink":"http://zeco.oschina.io/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"un11fin11","updated":"un11fin11","comments":true,"path":"categories/index.html","permalink":"http://zeco.oschina.io/categories/index.html","excerpt":"","text":""},{"title":"标签云","date":"un00fin00","updated":"un00fin00","comments":true,"path":"tags/index.html","permalink":"http://zeco.oschina.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"LinuxCon引发的思考","slug":"futrue_road","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2017/06/28/futrue_road/","link":"","permalink":"http://zeco.oschina.io/2017/06/28/futrue_road/","excerpt":"2017年的夏天，LinuxCon第一次在中国召开，一年之前，我对这个消息一点反应也没有，现在却突兀的觉得有些触动，似乎某些不起眼的东西就发生在你身边，这世界上没有谁能知道这对未来代表了什么","text":"2017年的夏天，LinuxCon第一次在中国召开，一年之前，我对这个消息一点反应也没有，现在却突兀的觉得有些触动，似乎某些不起眼的东西就发生在你身边，这世界上没有谁能知道这对未来代表了什么，我不知道，你也不知道，但世间的事总是这样，不是等你准备好了再发生，而是有着它自己的规律，就那么生长着，盛放着，爆发着，然后死亡。 闲扯了一堆没头没脑的话，就像我近来混沌的头脑，里面填了一些东西，反倒不如当初清醒，不知道自己需要什么了。 然而，今天突然看到linus在和Dirk Hohndel的炉边谈话里的这么一句话 For me, I was always self-motivated and knew what I wanted to do. I was never told what I should look at doing. I’m not sure my example is the right thing for people to follow. There are a ton of open source projects and, if you are a beginning programmer, find something you’re interested in that you can follow for more than just a few weeks. Get to know the code so well that you get to the point where you are an expert on a code piece. It doesn’t need to be the whole project. No one is an expert on the whole kernel, but you can know an area well. If you can be part of a community and set up patches, it’s not just about the coding, but about the social aspect of open source. You make connections and improve yourself as a programmer. You are basically showing off – I made these improvements, I’m capable of going far in my community or job. You’ll have to spend a certain amount of time to learn a project, but there’s a huge upside – not just from a career aspect, but having an amazing project in your life. 对于其中it’s not just about the coding，you make connections 这两句话话，感触尤深。 毫无波澜的工作生活往往让我们忘记一些东西，有些时候我们通过游戏，狂欢来消磨忘记它们，但这世界上最稳固的东西就是我们每天都要面对的日出日落，不论怎么逃避，他就在那里等着你回去，等着你去面对他。 我也模糊的明晰了自己所需要的东西，我不需要在每一个方面做得完美，但需要在某一个方面，做到令自己自豪的成绩，最后，一句话送给自己和看到这篇文章的你，the day and night, not only for small life,but for a gaint soul.","categories":[{"name":"闲扯","slug":"闲扯","permalink":"http://zeco.oschina.io/categories/闲扯/"},{"name":"职业思考","slug":"闲扯/职业思考","permalink":"http://zeco.oschina.io/categories/闲扯/职业思考/"}],"tags":[{"name":"about futrue","slug":"about-futrue","permalink":"http://zeco.oschina.io/tagcloud/about-futrue/"}]},{"title":"网站设计中的cache技术","slug":"cache_1","date":"un44fin44","updated":"un00fin00","comments":true,"path":"2017/03/02/cache_1/","link":"","permalink":"http://zeco.oschina.io/2017/03/02/cache_1/","excerpt":"页面缓存的原理大体是在浏览器对资源的第一次请求之后，把资源中的一部分存储在计算机的临时文件空间，再次请求的时候，按照特定的策略加载缓存的资源，减少HTTP请求次数与传输数据量，以此提高浏览效率，减少数据库和服务器的压力。 在网站设计时要充分考虑缓存，合理的利用缓存和静态技术可以大大的提高网站的运行速度。","text":"页面缓存的原理大体是在浏览器对资源的第一次请求之后，把资源中的一部分存储在计算机的临时文件空间，再次请求的时候，按照特定的策略加载缓存的资源，减少HTTP请求次数与传输数据量，以此提高浏览效率，减少数据库和服务器的压力。 在网站设计时要充分考虑缓存，合理的利用缓存和静态技术可以大大的提高网站的运行速度。 页面静态化将动态的页面生成静态的页面保存下来，但用户访问特定的页面时，直接将缓存好的静态页面返回给客户，省去了去数据库读取数据的过程，大大提高了网站的运行效率，降低了数据库的压力。 这里牺牲了网站数据的及时性，一般静态化缓存会和其他技术结合起来使用，而不是单独使用。 php里一般使用ob方法来实现页面静态缓存123ob_start();$content = ob_get_contents();ob_end_clean(); 页面部分静态化使用模块化设计，将不需要数据动态化的部分进行静态化，使用ob函数输出，也可以使用其他技术，如ESI。 一般即使动态的数据，如果数据变化不是十分频繁（我们通过设计固定时间来更新我们的数据），也可以通过一些标识（如：id）来保存静态化数据的页面，然后通过（id）来找到静态页面位置，当该页面被访问时，直接返回给用户。 这里需要设计人员分割是否需要动态数据显示 memcached（redis）缓存内存缓存，一般使用memcached或者redis来实现，将数据以key&amp;value的方式保存在内存中，因为数据缓存在内存中，无需再去数据库里读取，这样不仅大大减轻了数据库的压力，也一样提升网站的处理速度。 这里要注意memcached和redis的选用，如果只是单纯的key&amp;value的数据读取，那么memecached是不错的选择，如果对数据安全有要求，又需要比较复杂的数据存储形式，那么redis可能会更加适合你。具体详见Memcached和Redis 关于缓存缓存有时候是让人讨厌的东西，但对于网页设计确实不得不去考虑的东西，这里简单的列举了三个比较常见的缓存技术，当然还有许多其他的缓存技术，留待以后慢慢的整理。","categories":[{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/categories/php/"},{"name":"静态技术","slug":"php/静态技术","permalink":"http://zeco.oschina.io/categories/php/静态技术/"}],"tags":[{"name":"cache","slug":"cache","permalink":"http://zeco.oschina.io/tagcloud/cache/"},{"name":"php","slug":"php","permalink":"http://zeco.oschina.io/tagcloud/php/"}]},{"title":"使用phpspider开发的PHP爬虫","slug":"phpspider_1","date":"un22fin22","updated":"un66fin66","comments":true,"path":"2017/01/17/phpspider_1/","link":"","permalink":"http://zeco.oschina.io/2017/01/17/phpspider_1/","excerpt":"开源库出处：Github 开发框架phpspider是一个爬虫开发框架。使用本框架，你不用了解爬虫的底层技术实现，爬虫被网站屏蔽、有些网站需要登录或验证码识别才能爬取等问题。简单几行PHP代码，就可以创建自己的爬虫，利用框架封装的多进程Worker类库，代码更简洁，执行效率更高速度更快。","text":"开源库出处：Github 开发框架phpspider是一个爬虫开发框架。使用本框架，你不用了解爬虫的底层技术实现，爬虫被网站屏蔽、有些网站需要登录或验证码识别才能爬取等问题。简单几行PHP代码，就可以创建自己的爬虫，利用框架封装的多进程Worker类库，代码更简洁，执行效率更高速度更快。demo目录下有一些特定网站的爬取规则，只要你安装了PHP环境，代码就可以在命令行下直接跑。 糗事百科案例1234567891011121314151617181920212223242526272829303132$configs = array( &apos;name&apos; =&gt; &apos;糗事百科&apos;, &apos;domains&apos; =&gt; array( &apos;qiushibaike.com&apos;, &apos;www.qiushibaike.com&apos; ), &apos;scan_urls&apos; =&gt; array( &apos;http://www.qiushibaike.com/&apos; ), &apos;content_url_regexes&apos; =&gt; array( &quot;http://www.qiushibaike.com/article/\\d+&quot; ), &apos;list_url_regexes&apos; =&gt; array( &quot;http://www.qiushibaike.com/8hr/page/\\d+\\?s=\\d+&quot; ), &apos;fields&apos; =&gt; array( array( // 抽取内容页的文章内容 &apos;name&apos; =&gt; &quot;article_content&quot;, &apos;selector&apos; =&gt; &quot;//*[@id=&apos;single-next-link&apos;]&quot;, &apos;required&apos; =&gt; true ), array( // 抽取内容页的文章作者 &apos;name&apos; =&gt; &quot;article_author&quot;, &apos;selector&apos; =&gt; &quot;//div[contains(@class,&apos;author&apos;)]//h2&quot;, &apos;required&apos; =&gt; true ), ), ); $spider = new phpspider($configs); $spider-&gt;start(); 爬虫程序设计因为爬取的页面需要登录才能获取到关注者页面，所以从chrome登录之后把cookie拷贝下来给curl程序模拟登录。 使用两大独立循环进程组(用户索引进程组、用户详情进程组)，用的是php的pcntl扩展，封装了一个非常好用的类，使用起来和golang的携程也差不多了。 用户索引进程组先以一个用户为起点，抓取这个用户的关注了和关注者，然后合并入库，因为是多进程，所以当有两个进程在处理同一个用户入库的时候就会出现重复的用户，所以数据库用户名字段一定要建立唯一索引，当然也可以用redis这些第三方缓存来保证原子性，这个就见仁见智了。 用户详情进程组按照时间正序，拿到最先入库的用户抓取详情，并且把更新时间更新为当前时间，这样就可以变成一个死循环，程序可以无休止的跑，不断的循环更新用户信息。 程序运行过程中出现的错误,因为网站会给数据强制gzip压缩，需要通过解压来获取有效数据 $content = substr($content, 10);$content = gzinflate($content));curl_setopt( self::$ch, CURLOPT_ENCODING, ‘gzip’ ); 获取数据的作用（感想）将一堆数据进行分类分析，通过数据的分布可以分析出平常不易看出来的规律，对于我们的决策有很大的指引意义。 原文出处：event poll","categories":[{"name":"数据技术","slug":"数据技术","permalink":"http://zeco.oschina.io/categories/数据技术/"},{"name":"爬虫","slug":"数据技术/爬虫","permalink":"http://zeco.oschina.io/categories/数据技术/爬虫/"}],"tags":[{"name":"拿来的东西","slug":"拿来的东西","permalink":"http://zeco.oschina.io/tagcloud/拿来的东西/"},{"name":"爬虫","slug":"爬虫","permalink":"http://zeco.oschina.io/tagcloud/爬虫/"}]},{"title":"redis vs memecached","slug":"redis_vs_mem","date":"un66fin66","updated":"un11fin11","comments":true,"path":"2016/12/03/redis_vs_mem/","link":"","permalink":"http://zeco.oschina.io/2016/12/03/redis_vs_mem/","excerpt":"Redis 是由来自意大利西西里岛的Salvatore Sanfilippo开发的一款开源的，使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库。 这里不得不说一个勤奋的作者对一款开源软件是一件很重要的事，时至今日，与前辈memcached相比，Redis在大多数方面已经完成了全面的超越。 下面摘录一段在stackoverflow上看到关于redis的文章。","text":"Redis 是由来自意大利西西里岛的Salvatore Sanfilippo开发的一款开源的，使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库。 这里不得不说一个勤奋的作者对一款开源软件是一件很重要的事，时至今日，与前辈memcached相比，Redis在大多数方面已经完成了全面的超越。 下面摘录一段在stackoverflow上看到关于redis的文章。 Redis is more powerful, more popular, and better supported than memcached. Memcached can only do a small fraction of the things Redis can do. Redis is better even where their features overlap.For anything new, use Redis. Redis拥有着更加强大和丰富的功能 Memcached vs Redis: Direct ComparisonBoth tools are powerful, fast, in-memory data stores that are useful as a cache. Both can help speed up your application by caching database results, HTML fragments, or anything else that might be expensive to generate. 两者都是通过使用内存缓存，来帮助你的网站提速 Points to Consider1. Read/write speed: Both are extremely fast. Benchmarks vary by workload, versions, and many other factors but generally show redis to be as fast or almost as fast as memcached. I recommend redis, but not because memcached is slow. It&apos;s not. 2. Memory usage: Redis is better. memcached: You specify the cache size and as you insert items the daemon quickly grows to a little more than this size. There is never really a way to reclaim any of that space, short of restarting memcached. All your keys could be expired, you could flush the database, and it would still use the full chunk of RAM you configured it with. redis: Setting a max size is up to you. Redis will never use more than it has to and will give you back memory it is no longer using. I stored 100,000 ~2KB strings (~200MB) of random sentences into both. Memcached RAM usage grew to ~225MB. Redis RAM usage grew to ~228MB. After flushing both, redis dropped to ~29MB and memcached stayed at ~225MB. They are similarly efficient in how they store data, but only one is capable of reclaiming it. 3. Disk I/O dumping: A clear win for redis since it does this by default and has very configurable persistence. Memcached has no mechanisms for dumping to disk without 3rd party tools. 4. Scaling: Both give you tons of headroom before you need more than a single instance as a cache. Redis includes tools to help you go beyond that while memcached does not. 我们需要从以下4点来分析使用redis的优势 读写速度 内存使用 磁盘I/O dumping 缩放 memcachedMemcached is a simple volatile cache server. It allows you to store key/value pairs where the value is limited to being a string up to 1MB. It’s good at this, but that’s all it does. You can access those values by their key at extremely high speed, often saturating available network or even memory bandwidth. When you restart memcached your data is gone. This is fine for a cache. You shouldn’t store anything important there. If you need high performance or high availability there are 3rd party tools, products, and services available. redisRedis can do the same jobs as memcached can, and can do them better. Redis can act as a cache as well. It can store key/value pairs too. In redis they can even be up to 512MB. You can turn off persistence and it will happily lose your data on restart too. If you want your cache to survive restarts it lets you do that as well. In fact, that’s the default. It’s super fast too, often limited by network or memory bandwidth. If one instance of redis/memcached isn’t enough performance for your workload, redis is the clear choice. Redis includes cluster support and comes with high availability tools (redis-sentinel) right “in the box”. Over the past few years redis has also emerged as the clear leader in 3rd party tooling. Companies like Redis Labs, Amazon, and others offer many useful redis tools and services. The ecosystem around redis is much larger. The number of large scale deployments is now likely greater than for memcached. 从支持数据大小，速度，以及第三方支持来看两者的区别 The Redis SupersetRedis is more than a cache. It is an in-memory data structure server. Below you will find a quick overview of things Redis can do beyond being a simple key/value cache like memcached. Most of redis’ features are things memcached cannot do. Redis不仅仅作为一个cache来使用，他还是一个能够永久保存数据的NoSQL数据库 DocumentationRedis is better documented than memcached. While this can be subjective, it seems to be more and more true all the time. redis.io is a fantastic easily navigated resource. It lets you try redis in the browser and even gives you live interactive examples with each command in the docs. There are now 2x as many stackoverflow results for redis as memcached. 2x as many Google results. More readily accessible examples in more languages. More active development. More active client development. These measurements might not mean much individually, but in combination they paint a clear picture that support and documentation for redis is greater and much more up-to-date. Redis有着更丰富的文档支持 PersistenceBy default redis persists your data to disk using a mechanism called snapshotting. If you have enough RAM available it’s able to write all of your data to disk with almost no performance degradation. It’s almost free! In snapshot mode there is a chance that a sudden crash could result in a small amount of lost data. If you absolutely need to make sure no data is ever lost, don’t worry, redis has your back there too with AOF (Append Only File) mode. In this persistence mode data can be synced to disk as it is written. This can reduce maximum write throughput to however fast your disk can write, but should still be quite fast. There are many configuration options to fine tune persistence if you need, but the defaults are very sensible. These options make it easy to setup redis as a safe, redundant place to store data. It is a real database. Many Data TypesMemcached is limited to strings, but Redis is a data structure server that can serve up many different data types. It also provides the commands you need to make the most of those data types. Strings (commands)Simple text or binary values that can be up to 512MB in size. This is the only data type redis and memcached share, though memcached strings are limited to 1MB. Redis gives you more tools for leveraging this datatype by offering commands for bitwise operations, bit-level manipulation, floating point increment/decrement support, range queries, and multi-key operations. Memcached doesn’t support any of that. Strings are useful for all sorts of use cases, which is why memcached is fairly useful with this data type alone. Hashes (commands)Hashes are sort of like a key value store within a key value store. They map between string fields and string values. Field-&gt;value maps using a hash are slightly more space efficient than key-&gt;value maps using regular strings. Hashes are useful as a namespace, or when you want to logically group many keys. With a hash you can grab all the members efficiently, expire all the members together, delete all the members together, etc. Great for any use case where you have several key/value pairs that need to grouped. One example use of a hash is for storing user profiles between applications. A redis hash stored with the user ID as the key will allow you to store as many bits of data about a user as needed while keeping them stored under a single key. The advantage of using a hash instead of serializing the profile into a string is that you can have different applications read/write different fields within the user profile without having to worry about one app overriding changes made by others (which can happen if you serialize stale data). Lists (commands)Redis lists are ordered collections of strings. They are optimized for inserting, reading, or removing values from the top or bottom (aka: left or right) of the list. Redis provides many commands for leveraging lists, including commands to push/pop items, push/pop between lists, truncate lists, perform range queries, etc. Lists make great durable, atomic, queues. These work great for job queues, logs, buffers, and many other use cases. Sets (commands)Sets are unordered collections of unique values. They are optimized to let you quickly check if a value is in the set, quickly add/remove values, and to measure overlap with other sets. These are great for things like access control lists, unique visitor trackers, and many other things. Most programming languages have something similar (usually called a Set). This is like that, only distributed. Redis provides several commands to manage sets. Obvious ones like adding, removing, and checking the set are present. So are less obvious commands like popping/reading a random item and commands for performing unions and intersections with other sets. Sorted Sets (commands)Sorted Sets are also collections of unique values. These ones, as the name implies, are ordered. They are ordered by a score, then lexicographically. This data type is optimized for quick lookups by score. Getting the highest, lowest, or any range of values in between is extremely fast. If you add users to a sorted set along with their high score, you have yourself a perfect leader-board. As new high scores come in, just add them to the set again with their high score and it will re-order your leader-board. Also great for keeping track of the last time users visited and who is active in your application. Storing values with the same score causes them to be ordered lexicographically (think alphabetically). This can be useful for things like auto-complete features. Many of the sorted set commands are similar to commands for sets, sometimes with an additional score parameter. Also included are commands for managing scores and querying by score. Redis丰富的数据类型支持 Geo Redis has several commands for storing, retrieving, and measuring geographic data. This includes radius queries and measuring distances between points. Technically geographic data in redis is stored within sorted sets, so this isn’t a truly separate data type. It is more of an extension on top of sorted sets. Bitmap and HyperLogLogLike geo, these aren’t completely separate data types. These are commands that allow you to treat string data as if it’s either a bitmap or a hyperloglog. Bitmaps are what the bit-level operators I referenced under Strings are for. This data type was the basic building block for reddit’s recent collaborative art project: r/Place. HyperLogLog allows you to use a constant extremely small amount of space to count almost unlimited unique values with shocking accuracy. Using only ~16KB you could efficiently count the number of unique visitors to your site, even if that number is in the millions. Transactions and AtomicityCommands in redis are atomic, meaning you can be sure that as soon as you write a value to redis that value is visible to all clients connected to redis. There is no wait for that value to propagate. Technically memcached is atomic as well, but with redis adding all this functionality beyond memcached it is worth noting and somewhat impressive that all these additional data types and features are also atomic. While not quite the same as transactions in relational databases, redis also has transactions that use “optimistic locking” (WATCH/MULTI/EXEC). PipeliningRedis provides a feature called ‘pipelining’. If you have many redis commands you want to execute you can use pipelining to send them to redis all-at-once instead of one-at-a-time. Normally when you execute a command to either redis or memcached, each command is a separate request/response cycle. With pipelining, redis can buffer several commands and execute them all at once, responding with all of the responses to all of your commands in a single reply. This can allow you to achieve even greater throughput on bulk importing or other actions that involve lots of commands. Pub/SubRedis has commands dedicated to pub/sub functionality, allowing redis to act as a high speed message broadcaster. This allows a single client to publish messages to many other clients connected to a channel. Redis does pub/sub as well as almost any tool. Dedicated message brokers like RabbitMQ may have advantages in certain areas, but the fact that the same server can also give you persistent durable queues and other data structures your pub/sub workloads likely need, Redis will often prove to be the best and most simple tool for the job. Lua ScriptingYou can kind of think of lua scripts like redis’s own SQL or stored procedures. It’s both more and less than that, but the analogy mostly works. Maybe you have complex calculations you want redis to perform. Maybe you can’t afford to have your transactions roll back and need guarantees every step of a complex process will happen atomically. These problems and many more can be solved with lua scripting. The entire script is executed atomically, so if you can fit your logic into a lua script you can often avoid messing with optimistic locking transactions. ScalingAs mentioned above, redis includes built in support for clustering and is bundled with its own high availability tool called redis-sentinel. ConclusionWithout hesitation I would recommend redis over memcached for any new projects, or existing projects that don’t already use memcached. The above may sound like I don’t like memcached. On the contrary: it is a powerful, simple, stable, mature, and hardened tool. There are even some use cases where it’s a little faster than redis. I love memcached. I just don’t think it makes much sense for future development. Redis does everything memcached does, often better. Any performance advantage for memcached is minor and workload specific. There are also workloads for which redis will be faster, and many more workloads that redis can do which memcached simply can’t. The tiny performance differences seem minor in the face of the giant gulf in functionality and the fact that both tools are so fast and efficient they may very well be the last piece of your infrastructure you’ll ever have to worry about scaling. There is only one scenario where memcached makes more sense: where memcached is already in use as a cache. If you are already caching with memcached then keep using it, if it meets your needs. It is likely not worth the effort to move to redis and if you are going to use redis just for caching it may not offer enough benefit to be worth your time. If memcached isn’t meeting your needs, then you should probably move to redis. This is true whether you need to scale beyond memcached or you need additional functionality. 所感翻译无能啊，英文水平略显不足，还有就是专业纵深也不够，一些名词直接看不懂是什么意思，文章看到后半段的时候，脑子一片空白，虽然看得懂小半的意思，但是却不能理解具体的区别作用。 还有文章作者最后的一句话，I love memcached. I just don’t think it makes much sense for future development.莫名的让人有些难言的干涩，也摘下来共勉吧。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"nosql","slug":"数据库/nosql","permalink":"http://zeco.oschina.io/categories/数据库/nosql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://zeco.oschina.io/tagcloud/redis/"},{"name":"nosql","slug":"nosql","permalink":"http://zeco.oschina.io/tagcloud/nosql/"},{"name":"memcached","slug":"memcached","permalink":"http://zeco.oschina.io/tagcloud/memcached/"}]},{"title":"分布式服务器集群架构方案思考","slug":"zfenbu_1","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2016/07/15/zfenbu_1/","link":"","permalink":"http://zeco.oschina.io/2016/07/15/zfenbu_1/","excerpt":"网上看到一个分布式的服务器集群架构，看了下觉得自己对于分布式的理解清晰了不少，于是摘录下来以免丢失。","text":"网上看到一个分布式的服务器集群架构，看了下觉得自己对于分布式的理解清晰了不少，于是摘录下来以免丢失。原文出处：分布式服务器集群架构方案思考 by 夏日小草 大型网站演化简单说，分布式是以缩短单个任务的执行时间来提升效率的，而集群则是通过提高单位时间内执行的任务数来提升效率。 集群主要分为：高可用集群(High Availability Cluster)，负载均衡集群(Load Balance Cluster，nginx即可实现)，科学计算集群(High Performance Computing Cluster)。 分布式是指将不同的业务分布在不同的地方；而集群指的是将几台服务器集中在一起，实现同一业务。分布式中的每一个节点，都可以做集群。 而集群并不一定就是分布式的。 之前在网上看到一篇关于大型网站演化的博客。page 每个大型网站都会有不同的架构模式，而架构内容也就是在处理均衡负载，缓存，数据库，文件系统等，只是在不同的环境下，不同的条件下，架构的模型不一样，目的旨在提高网站的性能。 最初的架构只有应用程序，数据库，文件服务。到后来，分布式服务、集群架设。 关于均衡负载方案在上一篇，《Nginx反向代理实现均衡负载》讨论过过的nginx现实均衡负载方案，这里选择另一种HAProxy+Keepalived双机高可用均衡负载方案。 HAProxy是免费、极速且可靠的用于为TCP和基于HTTP应用程序提供高可用、负载均衡和代理服务的解决方案，尤其适用于高负载且需要持久连接或7层处理机制的web站点。 不论是Haproxy还是Keepalived甚至是上游服务器均提高生产力并增强可用性,也就是如下架构中Haproxy,Keepalived,Httpd服务器任意宕机一台服务还是可以正常运行的。 HAProxy的优点： 1、HAProxy是支持虚拟主机的，可以工作在4、7层(支持多网段)； 2、能够补充Nginx的一些缺点比如Session的保持，Cookie的引导等工作； 3、支持url检测后端的服务器； 4、本身仅仅就只是一款负载均衡软件；单纯从效率上来讲HAProxy更会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的； 5、HAProxy可以对Mysql读进行负载均衡，对后端的MySQL节点进行检测和负载均衡； 关于Redis缓存方案缓存分为服务器缓存和应用程序缓存。 关于应用程序内缓存，已经在Jue后台框架里面做了模块处理了。 关于服务器缓存，主要缓存服务器文件，减少服务器和php交互，减少均衡负载服务器和应用程序服务器交互。 缓存里面有一种典型的memcached，现在用的多的是redis轻量级缓存方案。 关于memcached与redis，看这篇 《Memcached vs Redis?》 Redis主要将数据存储在各种格式：列表，数组，集合和排序集，一次能接受多个命令，阻塞读写，等待直到另一个进程将数据写入高速缓存。一篇关于Reids缓存方案。《高可用、开源的Redis缓存集群方案》 关于NoSQL快速存储方案NoSQL在这里的使用价值是处理一些琐事，比如用户个人网站的一些css值，height,width,color等等的小而繁多的数据，采用NoSQL旨在提升数据库速度，减少对MySQL的SELECT请求。 关于NoSQL的方案很多了，选一个简单的MongDB好了。 关于分布式MySQL方案(做分布式MySQL还没尝试过，初期也不清楚mysql所需要的压力，所以第一期不打算做分布式MySQL) 《标准MySQL数据库外的5个开源兼容方案》 分布式集群方案综合起来，大致就是如下模型，初探分布式架构，还有很多要修改的，待续，时时更新中… 观后所感一个完整的架构设计并不是一件简单的事，其中涉及的知识很多，我也心知一口气吃不成胖子，更多的问题留待自己有更加成熟的思考和理解之后，再来整理","categories":[{"name":"分布式部署","slug":"分布式部署","permalink":"http://zeco.oschina.io/categories/分布式部署/"}],"tags":[{"name":"拿来的东西","slug":"拿来的东西","permalink":"http://zeco.oschina.io/tagcloud/拿来的东西/"},{"name":"分布式","slug":"分布式","permalink":"http://zeco.oschina.io/tagcloud/分布式/"}]},{"title":"四种基本的排序算法学习总结","slug":"algorithm_1","date":"un44fin44","updated":"un55fin55","comments":true,"path":"2016/06/23/algorithm_1/","link":"","permalink":"http://zeco.oschina.io/2016/06/23/algorithm_1/","excerpt":"都说算法是程序的核心,但是作为一个半路出家的程序员，情不自禁的有点畏惧这个东西，但是今天却被教训了一顿，因为亲眼目睹前辈以一个简单的排序方法提升了好几倍的查询速度，好吧，决心一点点的啃下算法的硬骨头，提升一点自己coding的视野和高度。","text":"都说算法是程序的核心,但是作为一个半路出家的程序员，情不自禁的有点畏惧这个东西，但是今天却被教训了一顿，因为亲眼目睹前辈以一个简单的排序方法提升了好几倍的查询速度，好吧，决心一点点的啃下算法的硬骨头，提升一点自己coding的视野和高度。 今天就整理一下最简单的四个排序算法：冒泡、快速、选择、插入。快速排序对一个乱序的数组，每次选择一个指定位置的元素（一般是第一个），每次scan，将被选择的元素作为基准，将乱序的数组分为大小两部分，后续递归剩下的部分，直到分割出来的数组长度不可再分。具体举例如下12345678910111213141516171819202122function quick_sort($arr) &#123; $length = count($arr); if($length &lt;= 1) &#123; //递归出口 return $arr; &#125; $base_array = $arr[0]; //选择第一个元素作为基准 //初始化两个数组,保证每次递归签数组清空 $left_array = array(); //小于基准的 $right_array = array(); //大于基准的 for($i=1; $i&lt;$length; $i++) &#123; if($base_num &gt; $arr[$i]) &#123;//放入左边数组 $left_array[] = $arr[$i]; &#125; else &#123; //放入右边 $right_array[] = $arr[$i]; &#125; &#125; //递归排序 $left_array = quick_sort($left_array); $right_array = quick_sort($right_array); //合并数组 return array_merge($left_array, array($base_num), $right_array); &#125; 冒泡排序冒泡排序基本是一个programer要学习的第一个算法，但是简单不代表他真的就low，就像递归和迭代算法一样，并没有那个算法是最优的，而是适当的环境下选择适当的算法。冒泡算法，通过两层循环，从第一个元素开始，每次对比相邻的元素，依据他们的大小和需要的排序决定他们的位置，最后获得一个重新排序过的数组。具体举例如下12345678910111213function bubbleSort($arr)&#123; $len=count($arr); for($i=1;$i&lt;$len;$i++) &#123; //第一层循环控制层数 for($k=0;$k&lt;$len-$i;$k++)&#123; //第二层循环用来控制级数 if($arr[$k]&gt;$arr[$k+1])&#123; $tmp=$arr[$k+1]; $arr[$k+1]=$arr[$k]; $arr[$k]=$tmp; &#125; &#125; &#125; return $arr; &#125; 选择排序选择排序是通过假设一个假值，然后获得这个假值的位置（每次选择最大或者最小的树），寻找到他的位置，排列出一个固定顺序的数组。具体举例如下12345678910111213141516171819202122function selectSort($arr) &#123; //双重循环完成，外层控制轮数，内层控制比较次数 $len=count($arr); for($i=0; $i&lt;$len-1; $i++) &#123; //假设最小的值的位置 $p = $i; for($j=$i+1; $j&lt;$len; $j++) &#123; //比较，发现更小的,记录下最小值的位置；并且在下次比较时采用已知的最小值进行比较。 if($arr[$p] &gt; $arr[$j]) &#123; $p = $j; &#125; &#125; //已经确定了当前的最小值的位置，保存到$p中。如果发现最小值的位置与当前假设的位置$i不同，则位置互换即可。 if($p != $i) &#123; $tmp = $arr[$p]; $arr[$p] = $arr[$i]; $arr[$i] = $tmp; &#125; &#125; //返回最终结果 return $arr; &#125; 插入排序假设一个数组是有序的，现在要把第n个数插到前面的有序数中，使得这n个数也是排好顺序的。如此反复循环，直到全部排好顺序。具体举例如下123456789101112131415161718function insertSort($arr) &#123; $len=count($arr); for($i=1, $i&lt;$len; $i++) &#123; $tmp = $arr[$i]; //内层循环控制，比较并插入 for($j=$i-1;$j&gt;=0;$j--) &#123; if($tmp &lt; $arr[$j]) &#123; //发现插入的元素要小，交换位置，将后边的元素与前面的元素互换 $arr[$j+1] = $arr[$j]; $arr[$j] = $tmp; &#125; else &#123; //如果碰到不需要移动的元素，由于是已经排序好是数组，则前面的就不需要再次比较了。 break; &#125; &#125; &#125; return $arr; &#125;","categories":[{"name":"算法学习","slug":"算法学习","permalink":"http://zeco.oschina.io/categories/算法学习/"}],"tags":[{"name":"排序","slug":"排序","permalink":"http://zeco.oschina.io/tagcloud/排序/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://zeco.oschina.io/tagcloud/Algorithm/"}]},{"title":"git操作(I) 提交到远程仓库","slug":"git_1","date":"un55fin55","updated":"un22fin22","comments":true,"path":"2016/05/27/git_1/","link":"","permalink":"http://zeco.oschina.io/2016/05/27/git_1/","excerpt":"目前市面有许多第三方git操作软件，比如我曾经用过的Source Tree，就是一款上手和使用起来都不错的工具，但时间一长，总觉得差点微妙的feel，于是又灰溜溜的决定把命令行捡回来，写个系列的使用记录。","text":"目前市面有许多第三方git操作软件，比如我曾经用过的Source Tree，就是一款上手和使用起来都不错的工具，但时间一长，总觉得差点微妙的feel，于是又灰溜溜的决定把命令行捡回来，写个系列的使用记录。 创建一个本地仓库 现在，有一个尚未开始的项目，那么进入你项目存放的目录12$ cd XXX$ git init 这样你就获得了一个仓库，并且处于当前master branch上 或者通过clone一个远程的仓库到本地12$ cd XXX$ git clone url:repositories [repo&apos;s name] 需要注意的是，这个本地文件夹（XXX）要是一个空的文件夹，否则会报错 完成文件提交 创建一个本地文件1$ vim ./hello_git.md 在文件输入你想加入的内容1~ hello git :) 保存后退出 添加文件到暂存区，提交文件123456789使用$ git status查看当前文件状态，出现未添加文件，使用$ git add ./hello_git.md再查看文件状态‘$ git status这里显示文件暂存，然后准备commit$ git commit -m &apos;备注信息&apos;再次查看工作区时，显示tree clean，说明提交成功 你也可以使用 git status -s 命令查看更为详细的状态也可以通过 git log 来查看每次commit 关联远程仓库git 属于分布式版本控制系统,每个电脑就相当于一个仓库，但如果你有一个geek的心，你可以将你的代码分享到一个远程的托管系统上，其中的代表有世界上最大的同性交友网站github，还有国内的两个不错的托管平台coding和码云。 我们可以通过remote命令将自己的代码托管在远程的代码托管平台上 查看关联的远程仓库1$ git remote -v 获得已经关联的远程仓库2 . 配置本地账户信息配置一个本地的用户信息，使得仓库中的操作是由谁做出的12$ git config --global user.name &quot;your_username&quot; $ git config --global user.email your_email@domain.com 生成ssh秘钥1$ ssh-keygen -t rsa -C &quot;your_email@domain.com&quot; 输入命令之后会提示三次确认，一直回车确定，最后提示创建成功，然后进入主文件查看生成的id_rsa和id_rsa.pub，将id_rsa.pub公钥打开，添加到远程创建的长裤中 打开公钥使用编辑器，否则可能会影响编码，导致添加失败 添加远程仓库地址12$ git remote add origin https://your_username@bitbucket.org/your_username/name_of_remote_repository.git $ git push origin master 显示push成功之后，就可以再远程仓库里看到你提交的代码了 如果使用的clone仓库，那么本地默认存在远程仓库地址，只需要添加ssh公钥认证之后就可以提交代码到远程仓库了 如果你看了这些还是不太了解提交的过程，可以参考git官方给出的操作book","categories":[{"name":"工具","slug":"工具","permalink":"http://zeco.oschina.io/categories/工具/"},{"name":"git","slug":"工具/git","permalink":"http://zeco.oschina.io/categories/工具/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://zeco.oschina.io/tagcloud/git/"}]},{"title":"好玩的ReidsCycle","slug":"rediscycle","date":"un11fin11","updated":"un11fin11","comments":true,"path":"2016/05/16/rediscycle/","link":"","permalink":"http://zeco.oschina.io/2016/05/16/rediscycle/","excerpt":"最近，突发奇想的想要设计一个延时触发的项目，上网翻腾了一把，着实发现了不少有意思的东西，这里附上感觉不错的两处：一、二","text":"最近，突发奇想的想要设计一个延时触发的项目，上网翻腾了一把，着实发现了不少有意思的东西，这里附上感觉不错的两处：一、二 比较常见的方案使用cron定时任务，但是cron设定任务最低的精度到minute,某些时候，我们需要的可能是精确到second级别的定时计划。 再者，当表单数据量巨大时，使用cron进行轮询的效率就很低。 因为，每次轮询都要扫一次库，之前执行过的记录仍会被扫描，这样的效率就会下降 使用redis实现高效的延时设计参见一中的设计，这里包含两个重要的数据结构： 环形队列，一个头尾相接数组，其包含3600个slot的环形队列（一环设计为1h==60min==3600s，cycle=n?） work_ task, 置于指定时间点的 [‘set’=&gt;[cycle,dot,work_task]] timer, 一个和环形队列对应的timer是必须的,每当timer变化设定的dot值时，环形队列的指针（index）便移动一位，同事检测当前dot上的set集，对比集中cycle，如果对应上，便执行此点上的work_ task 这里我们设定一个例子： slot(3600,dot=1s) timer=dot=1s set=[task_1=&gt;[‘cycle’=&gt;3,’dot’=&gt;667,’task’=&gt;function(){echo ‘do it’}]] 任务执行的逻辑如下：123(cycle=1,dot=667)--&gt;1 != 3 &amp;&amp; 667 == 667 continue(cycle=2,dot=667)--&gt;2 != 3 &amp;&amp; 667 == 667 continue(cycle=3,dot=667)--&gt;3 == 3 &amp;&amp; 667 == 667 do work_task--&gt;echo do it--&gt;delete work_task 从任务的逻辑不难看出， 每次指针移动时，只需要查看当前dot上是否有set如果有set集则进行判断cycle值，决定是否执行任务，无需轮询全部任务，效率提升 每个订单任务被执行之后，即刻更新任务数据，一个任务只执行一次 时效性自定义，再不影响系统性能的情况下，就能够做到足够高的精确度","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"nosql","slug":"数据库/nosql","permalink":"http://zeco.oschina.io/categories/数据库/nosql/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://zeco.oschina.io/tagcloud/redis/"},{"name":"环形队列","slug":"环形队列","permalink":"http://zeco.oschina.io/tagcloud/环形队列/"},{"name":"延时设计","slug":"延时设计","permalink":"http://zeco.oschina.io/tagcloud/延时设计/"}]},{"title":"PHP+salt加密","slug":"php+salt","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2016/05/13/php+salt/","link":"","permalink":"http://zeco.oschina.io/2016/05/13/php+salt/","excerpt":"","text":"最近又听闻用户信息泄露的新闻，此前，作为it小白的时候，实在不知道怎么处理这样的变故，而泄漏的信息如果被黑客利用，通过撞库的方法就可破解大多数用户的密码，毕竟用户偏向于使用自己简单容易记住的密码，也不会多去改变他们，这时，如果我们开发人员还使用单一的加密方式，那是十分不明智的。 MD5加盐加密我们对md5()这个函数再是熟悉不过了，都使用过md5()对用户密码进行加密处理，这样做没有错，因为MD5加密不可逆，但是这样做的安全性还是很低的，因为很多网站的用户数据都是用md5进行加密处理的，而且网上也有许多人为整理出来的常用的MD5加密库，而从CSDN当时泄漏出来的用户信息来看，即便是作为与网络打交道的程序员，其中也有许多使用的是简单的密码组合，及其容易被匹配出来，再者，对于黑客而言，其破解密码手段通过撞库，简单的密码组合就大大增大了密码被破解的几率。 直接去开发一个新的算法来加密，从现实的角度是不实际的，那既然从算法的角度无法实现，那么我们就可以从入口的数据下手，将用户的密码添加一些佐料，这样即便是简单的密码，也会因为我们的加密，提升了密码组合的复杂程度，不会那么轻易的被破解，这就是所谓的加salt。 12345678910$salt = get_salt(SALT); //我们通过设置不同的SALT值来获取不同等级的加密$password = &apos;we2134sda&apos;; //用户处获取的明文密码$md5_password = md5(&apos;your_site&apos;.$password.$salt);//最终加密后密码function get_salt（$param） &#123; $salt = &apos;&apos;; for ($i = 0;$i &lt; $param; $i++) &#123; $salt .= chr(mt_rand(13,$param*12+13)); &#125; return $salt;&#125; 这里我们使用md5加随机生成的salt来增强加密后的密码安全性，然后我们记录下salt值，在用户注册的时候和密码一起生成并保存到数据库中，用户登录验证的时候再把密码和盐值一起组合验证，通过这样的手段就可以加强密码的安全性。","categories":[{"name":"安全","slug":"安全","permalink":"http://zeco.oschina.io/categories/安全/"}],"tags":[{"name":"加密","slug":"加密","permalink":"http://zeco.oschina.io/tagcloud/加密/"},{"name":"md5","slug":"md5","permalink":"http://zeco.oschina.io/tagcloud/md5/"}]},{"title":"用hexo+github搭建一个静态blog","slug":"your blog","date":"un22fin22","updated":"un00fin00","comments":true,"path":"2016/05/03/your blog/","link":"","permalink":"http://zeco.oschina.io/2016/05/03/your blog/","excerpt":"关于怎么搭建blog的教程网上有一堆，挑一个比较详细的附上：如何搭建一个独立博客——简明Github Pages与Hexo教程这里就记录一些个人探过的一些小坑，以做前车之鉴。","text":"关于怎么搭建blog的教程网上有一堆，挑一个比较详细的附上：如何搭建一个独立博客——简明Github Pages与Hexo教程这里就记录一些个人探过的一些小坑，以做前车之鉴。 配置文件使用hexo搭建blog的时候系统默认了一个theme：landscape 我使用的是这个主题：ICARUS 下载之后要在主配置文件 _config.yaml里配置1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: hexo-theme-icarus //你所下载的主题仓库名称 而如果需要调整请在主题文件夹的 _config.yaml里配置 注意两者的位置主配置文件位于hexo文件的根目录中主题配置文件的目录位于hexo-&gt;themes-&gt;(your themes)-&gt;_config.yaml(一般出事文件还有后缀example，将他去掉就好了) 部署到github使用$ hexo d 就可以将生成好的静态文件部署到github上，但部署文件之前需要在主配置文件里配置123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/yourgithubname/yourgithubname.github.io.git branch: master 注意，这里的仓库名称一定要是你的：github用户名.github.io才会被github默认为pages 但需要注意的是，有些时候这样的配置不一定能够成功，因为在hexo3里使用https:// 会影响他的部署这里要将仓库地址repo改成1repo：git@github.com:username/username.github.io.git 如果还是失败，那就要检查SSH key是否添加成功，以及git的设置是否成功 关于MarkDown的书写使用hexo new post [post title],可以生成一篇新的文章，只需要到/source/_post/下就可以找到生成的title.md文件下面是标题文件的书写123456---title: blog标题categories: [一级分类，二级分类...]tags: [标签1,标签2,标签3...]thumbnail: url of image(缩略图的路径)--- 博文正文的书写惨遭markdown语法，markdown语法兼容html的语法，相信对学习过html标签都不是一件难事。这里再提一点，如果需要只显示部分的博文可以使用1&lt;!--more--&gt; 来分割显示和不显示的文章 关于pluginhexo官网上有许多优秀的plugin，我找了一个比较有意思的标签云插件按照readme一步一步配置就可以实现动态的tagcloud了 最后上一切的起源：hexo","categories":[{"name":"blog","slug":"blog","permalink":"http://zeco.oschina.io/categories/blog/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zeco.oschina.io/tagcloud/blog/"},{"name":"hexo","slug":"hexo","permalink":"http://zeco.oschina.io/tagcloud/hexo/"},{"name":"github","slug":"github","permalink":"http://zeco.oschina.io/tagcloud/github/"}]},{"title":"MySQL (III) 字段类型选择","slug":"mysql_uh3","date":"un66fin66","updated":"un44fin44","comments":true,"path":"2016/04/30/mysql_uh3/","link":"","permalink":"http://zeco.oschina.io/2016/04/30/mysql_uh3/","excerpt":"字段是组成一张表的最基本单元，但它们却是数据表设计时不得不严密考虑的部分，因为不恰当的字段类型选择，不仅容易造成数据库空间的浪费，以及冗余的产生，且在数据量巨大以及查询频繁的字段或表，将严重影响数据的读写速度。","text":"字段是组成一张表的最基本单元，但它们却是数据表设计时不得不严密考虑的部分，因为不恰当的字段类型选择，不仅容易造成数据库空间的浪费，以及冗余的产生，且在数据量巨大以及查询频繁的字段或表，将严重影响数据的读写速度。 Int（bigint，int，mediumint，smallint，tinyint）因为计算机本身只分辨数字型的数据，所以合理的使用整形字段可以提升数据读取的速度。 整形数据由大至小，从16个字节到1个字节不等，对于不同需求的表选择不同类型的int类型作为表的primary key是一个不错的选择。 这里还要提到的一点，虽然MySQL数据库提供了datetime类型的字段，但我们还是尽量以int类型的格式来存储时间戳的方法来保存时间数据，这样可以提高读取速度和减少I/O的开销。 在一些单选或者多选的字段里，也应当尽量避免使用枚举，适当的选用tinyint这样的短int数据是更好的选择。 Float（float，double，decimal）其实在浮点类型的数据中，decimal并不是作为数字存储在数据库中的，相反他是以我们’厌恶’的字符类型存在。 在这里，我们不得不来讨论一下浮点型数的一些缺点，单/双精度的数据类型在数据库中超过一定位数之后会出现失去精度的的情况，这个位数的大小在6位小数，而我们存储一些类似价格，额度等字段时，需要一种完全精确的记录，而decimal（65，30）如此长度的记录数完全满足我们的需求，这是浮点类型数据我们应当关心的部分。 字符（char，varchar）虽然字符类型的数据在读取时并不佳，但字符型的数据却是一个数据库不可忽略的部分。 char和varchar的不同体现在多个方面 1.char类型数据最大长度（255），而varchar类型数据的最大长度为（65535） 这里需要注意的是，varchar类型的数据长度存储的字节长度，即varchar类型的数据需要考虑编码的原因 2.两者最主要体现在两者侧重的方面不同，char类型数据属于定长数据，不论存储多少长度的数据，都占据一定的空间，而varchar数据则不论在表中设计多大的长度，都依照存储的字符串长度来决定占用的空间 3.因为varchar之所节省空间，是因为varchar经过了一层数据库的算法过滤，恰恰也是这层过滤，使得varchar类型的数据在读写速度上劣于char类型的数据。 总而言之，varchar和char两者，一个为了节省空间而浪费了时间，一个为了节省时间而浪费了空间，这就需要我们在设计表格的时候，谨慎的去考虑和取舍了。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"mysql","slug":"数据库/mysql","permalink":"http://zeco.oschina.io/categories/数据库/mysql/"}],"tags":[{"name":"字段优化","slug":"字段优化","permalink":"http://zeco.oschina.io/tagcloud/字段优化/"}]},{"title":"Mysql (II) 引擎","slug":"mysql_uh2","date":"un44fin44","updated":"un44fin44","comments":true,"path":"2016/04/21/mysql_uh2/","link":"","permalink":"http://zeco.oschina.io/2016/04/21/mysql_uh2/","excerpt":"这里，只简单的讨论两种常见的引擎：MyISAM和InnoDB的区别。","text":"这里，只简单的讨论两种常见的引擎：MyISAM和InnoDB的区别。 a. MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持. b. MyISAM类型的表强调的是性能，其执行数度比InnoDB类型更快. c. InnoDB不支持fulltext类型的索引. d. InnoDB 中不保存表的具体行数，也就是说，执行select count(*) from table时，InnoDB要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可. e. 对于auto_increment类型的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中，可以和其他字段一起建立联合索引。 f. delete from table时，InnoDB不会重新建立表，而是一行一行的删除。 g. load table from master操作对InnoDB是不起作用的，解决方法是首先把InnoDB表改成MyISAM表，导入数据后再改成InnoDB表，但是对于使用的额外的InnoDB特性(例如外键)的表不适用. h. MyISAM支持表锁，InnoDB支持行锁。 B-treeInnoDB的B-tree结构 如上图所示，InnoDB的搜索树由两层结构组成，每次对InnoDB引擎的表操作时，实际上是经过了两次的处理，先通过索引找到主键的位置，再获取对应主键的记录，这样不论是写入还是读取的速度都讲收到影响。 MyISAM的B-tree结构 MyISAM的结构决定了，每次读取和写入的时候不用考虑主键的顺序重排，所以MyISAM引擎的表的读取速度较InnoDB的快","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"mysql","slug":"数据库/mysql","permalink":"http://zeco.oschina.io/categories/数据库/mysql/"}],"tags":[{"name":"数据库引擎","slug":"数据库引擎","permalink":"http://zeco.oschina.io/tagcloud/数据库引擎/"}]},{"title":"Mysql (I) 数据表设计规范","slug":"mysql_uh1","date":"un00fin00","updated":"un44fin44","comments":true,"path":"2016/04/03/mysql_uh1/","link":"","permalink":"http://zeco.oschina.io/2016/04/03/mysql_uh1/","excerpt":"三范式&nbsp;&nbsp;&nbsp;&nbsp;俗话说，好的表结构是一个项目成功的一般，而在项目中我们一般按照三范式的规范来设计我们的表格。三范式的具体要求如下： 原子性（不可再分的字段） 非主键字段必须依赖主键 消除非主键之间的传递依赖","text":"三范式&nbsp;&nbsp;&nbsp;&nbsp;俗话说，好的表结构是一个项目成功的一般，而在项目中我们一般按照三范式的规范来设计我们的表格。三范式的具体要求如下： 原子性（不可再分的字段） 非主键字段必须依赖主键 消除非主键之间的传递依赖 &nbsp;&nbsp;&nbsp;&nbsp;当然，三范式在网上存在许多不同的版本，但不论是哪个版本的三范式，其中心思想都是一样的，那就是设计出比较合理的数据表，尽可能的减少代码的冗余。 但需要注意的是，代码的冗余只能减少，而不是消灭 具体设计思路如下： id name order order_name address weather price (非设计规范表) (拆分address字段保证设计表的原子性) -&gt; id name order order_name country province city weather price (将非主键依赖的weather的字段拆分出去) -&gt; id name order order_name country province city price (消除order和order_name之间的传递依赖，将非主键之间的传递依赖消除) -&gt; id name order country province city price 逆范式&nbsp;&nbsp;&nbsp;&nbsp;实际项目中，并不是完全按三范式的规范来设计表的结构，具体的项目中，逆范式的设计有时候可以简化sql的语句，提高sql语句的执行效率，此时逆范式的设计明显更有利于我们的项目，这个情况下使用逆范式的设计将更为科学。 例如，我们现在有两张表，一张分类表（category），一张商品表（goods），这里我们项目需要查询分类id、分类名称、商品数量三个字段，具体的sql语句如下：1select c.*,count(g.goods_id) goods_num from category as c left join goods as g on c.cat_id = g.cat_id group by c.cat_id; 可以看到，这个使用jion连表查询的sql语句相对于简单的表结构并不简单，而如果我们给分类表加上一个对应的商品数量字段，这样的sql语句将大大简化，这个时候我们大可不必严守三范式的设计思路，不妨使用逆范式的方式来设计这张表。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zeco.oschina.io/categories/数据库/"},{"name":"mysql","slug":"数据库/mysql","permalink":"http://zeco.oschina.io/categories/数据库/mysql/"}],"tags":[{"name":"表设计","slug":"表设计","permalink":"http://zeco.oschina.io/tagcloud/表设计/"}]}]}